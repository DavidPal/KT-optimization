\appendix
\section{Appendix}

\subsection{Proof of Theorems \ref{theo:1-d_reward} and \ref{theo:hilbert_reward}}

The following Lemma is from~\cite{McMahanO14} and reported here with our notation for completeness.
\begin{lemma}[Extremes]
\label{lemma:extremes}
Let $h:(-a,a) \to \R$ be an even concave twice-differentiable function that
satisfies $x \cdot h''(x) \le h'(x)$ for all $x \in [0,a)$. Then, if vectors
$u,v \in \H$ satisfy $\|u\| + \|v\| < a$, then
\begin{equation}
\label{equation:lemma-extremes-1}
\langle u, v \rangle + h(\|u + v\|) \ge \min \left\{ \|u\| \cdot \|v\| + h(\|v\| + \|u\|), \ - \|u\| \cdot \|v\| + h(\|u\| - \|v\|) \right\} \; .
\end{equation}
Futhermore, for any $b \in [0,a]$ and any $u, v \in \H$ such that $\|v\| < a - b$ and $\|u\| \le b$,
\begin{equation}
\label{equation:lemma-extremes-2}
\langle u, v \rangle + h(\|u + v\|) \ge \min \left\{ b \cdot \|v\| + h(\|v\| + b), \ - b \cdot \|v\| + h(\|u\| - b) \right\} \; .
\end{equation}
\end{lemma}
\begin{proof}[Proof of Lemma \ref{lemma:extremes}]
If $u$ or $v$ is zero, the inequality \eqref{equation:lemma-extremes-1} clearly holds. From now on we assume that
$u,v$ are non-zero. Let $c$ be the cosine of the angle of between $u$ and $v$.
More formally,
$$
c = \frac{\langle u, v \rangle}{\|u\| \cdot \|v\|} \; .
$$
With this notation, the left-hand side is
$$
\langle u, v \rangle + h(\|u + v\|) = c \|u\| \cdot \|v\|  + h(\sqrt{\|u\|^2 + \|v\|^2 + 2 c \|u\| \cdot \|v\|}) \; .
$$
We consider the last expression as a function of $c$ and we call it $f(c)$. The
inequality \eqref{equation:lemma-extremes-1} is equivalent to
$$
\forall c \in [-1,1] \qquad \qquad f(c) \ge \min \left\{f(+1), f(-1)\right\} \; .
$$
The last inequality is clearly true if $f:[-1,1] \to \R$ is concave. We now
check that $f$ is indeed concave, which we prove by showing that the second
derivative is non-positive. The first derivate of $f$ is
$$
f'(c) = \|u\| \cdot \|v\| + \frac{h'(\sqrt{\|u\|^2 + \|v\|^2 + 2 c \|u\| \cdot \|v\|}) \cdot \|u\| \cdot \|v\|}{\sqrt{\|u\|^2 + \|v\|^2 + 2 c \|u\| \cdot \|v\|}} \; .
$$
The second derivative of $f$ is
$$
f''(c) = \|u\|^2 \cdot \|v\|^2 \cdot \frac{h''(\sqrt{\|u\|^2 + \|v\|^2 + 2 c \|u\| \cdot \|v\|})  - \frac{1}{\sqrt{\|u\|^2 + \|v\|^2 + 2c \|u\| \cdot \|v\|}} \cdot h'(\sqrt{\|u\|^2 + \|v\|^2 + 2 c \|u\| \cdot \|v\|})  }{\|u\|^2 + \|v\|^2 + 2 c \|u\| \cdot \|v\|} \; .
$$
If we consider $x=\sqrt{\|u\|^2 + \|v\|^2 + 2 c \|u\| \cdot \|v\|}$, the
assumption $x \cdot h''(x) \le h'(x)$ implies that $f''(c)$ is non-positive.
This finishes the proof of the inequality \eqref{equation:lemma-extremes-1}.

Inequality \eqref{equation:lemma-extremes-2}, can be derived from inequality
\eqref{equation:lemma-extremes-1} as follows
\begin{align*}
\langle u, v \rangle + h(\|u + v\|)
& \ge \min_{u \in \H : \|u\| \le b} \min \left\{ \|u\| \cdot \|v\| + h(\|v\| + b), \ - \|u\| \cdot \|v\| + h(\|u\| - \|v\|) \right\} \\
& = \min_{z \in [0,b]} \min \left\{ z \cdot \|v\| + h(\|v\| + z), \ - z \cdot \|v\| + h(\|u\| - z) \right\} \\
& = \min_{z \in [-b,b]} z \cdot \|v\| + h(\|v\| + z) \\
& = \min \left\{ b \cdot \|v\| + h(\|v\| + b), \ - b \cdot \|v\| + h(\|u\| - b) \right\} \; .
\end{align*}
The inequality in the chain is the inequality \eqref{equation:lemma-extremes-1}.
The last equality follows since $g(z) = z \cdot \|v\| + h(\|v\| + z)$
is concave.
\end{proof}

We will also use the following Theorem from~\citet{McMahanO14}, that allows to recast the problem of proving an upper bound to the regret to the one of proving a lower bound to the \emph{wealth} of the algorithm.
The reward and regret view on online learning are equivalent: an algorithm guarantees low regret iff it guarantees high reward. The following Theorem makes this claim rigorous. Notice that the algorithm is exactly the same in the two setting.
\begin{theorem}[\citet{McMahanO14}]
  \label{thm:rrdual}
  Let $\Psi:\mathcal{H} \rightarrow (-\infty, +\infty]$ be a lower semicontinuos and convex function, with $dom \Psi \neq \emptyset$. An
  algorithm for the player guarantees
  \[
  \gain_n \geq \Psi\left(\sum_{t=1}^n g_t\right) - \epsilon \quad \quad \quad \textnormal{ for any } g_1, \dots, g_n
  \]
  for a constant $\epsilon \in \R$ if and only if it
  guarantees
  \begin{equation}\label{eq:regb}
  \qquad Regret_n(u) \leq \Psi^*(u) + \epsilon \quad \quad \quad \textnormal{ for all } u \in \fH \textnormal{ and } g_t \in \fH, \forall 1\leq t\leq n~.
  \end{equation}
\end{theorem}
The theorem implies that a betting algorithm can be used for online learning and vice-versa. However, as it was already stressed in \citet{McMahanO14}, the reward view has the big advantage of having one variable less, the competitor $u$.
Moreover, as we will show in the following, designing and analyzing algorithms in one of the two views could be much easier than in the other one.

\begin{proof}[Proof of Theorem~\ref{theo:hilbert_reward}]
  We will first prove a lower bound on the Wealth of the algorithm, that derive a regret bound using Theorem~\ref{thm:rrdual}.
  For simplicity denote by $f_t(\cdot)=f\left(\cdot, \{\norm{g_1}, \ldots, \norm{g_t}\}\right)$.
  We will prove the thesis by induction. The base case is verified from the first point of Assumption~\ref{assumption:1-d_algo}. The, we assume that 
  \[
  \epsilon + \sum_{t=1}^{n-1} \langle g_t, w_t \rangle \geq f_{n-1}\left( \norm{\sum_{t=1}^{n-1} g_t}\right),
  \]
  and we want to prove that 
  \[
  \epsilon + \sum_{t=1}^{n} \langle g_t, w_t \rangle \geq f_{n}\left( \norm{\sum_{t=1}^{n} g_t}\right)~.
  \]
  We have that
  \begin{align*}
  \epsilon + \sum_{t=1}^{n} &\langle g_t, w_t \rangle - f_n\left( \norm{\sum_{t=1}^{n} g_t}\right) \\
  &= \langle g_n, w_n \rangle + \epsilon + \sum_{t=1}^{n-1} \langle g_t, w_t \rangle - f_n\left( \norm{\sum_{t=1}^{n} g_t}\right)\\
  &= \left(1+\frac{b_n}{\norm{\theta_{n-1}}}\langle \theta_{n-1},g_n \rangle \right)\left(\sum_{t=1}^{n-1} \langle g_t, w_t \rangle +\epsilon \right) - f_n\left( \norm{\sum_{t=1}^{n} g_t}\right)\\
  &\geq \left(1+\frac{b_n}{\norm{\theta_{n-1}}}\langle \theta_{n-1},g_n \rangle \right) f_{n-1}\left( \norm{\sum_{t=1}^{n-1} g_t}\right) - f_n\left( \norm{\sum_{t=1}^{n} g_t}\right)\\
  &= \left(1+\frac{b_n}{\norm{\theta_{n-1}}}\langle \theta_{n-1},g_n \rangle \right) f_{n-1}\left( \norm{\sum_{t=1}^{n-1} g_t}\right) - f_n\left( \norm{g_n + \sum_{t=1}^{n-1} g_t}\right)\\
  %&= \left(1+\frac{b_n}{\norm{\theta_{n-1}}}\langle \theta_{n-1},g_n \rangle \right) f_{n-1}\left( \norm{\theta_{n-1}}\right) - f_n\left( \sqrt{\norm{\theta_{n-1}}^2 + \norm{g_n}^2 + 2 \langle \theta_{n-1}, g_n \rangle} \right)\\
  &\geq \min_{r\in \{-1,1\}} \left(1+ r\, b_n \norm{g_t}\right) f_{n-1}\left( \norm{\theta_{n-1}}\right) - f_n\left(\left| \norm{\theta_{n-1}} + r\norm{g_t}\right|\right)\\
  &= \min_{r\in \{-1,1\}} \left(1+ r\, b_n \norm{g_t}\right) f_{n-1}\left( \norm{\theta_{n-1}}\right) - f_n\left( \norm{\theta_{n-1}} + r \norm{g_t}\right)\\
  &\geq 0,
  \end{align*}
  where the first inequality comes from the induction hypothesis, the second one using Lemma~\ref{lemma:extremes} and the last one by the hypothesis on the \ac{MBA}.
  
  An application of Theorem~\ref{thm:rrdual} finishes the proof.
\end{proof}

\section{Proof of Theorem~\ref{theo:kt_is_mba}}

The following Lemma proves the second point of Assumption~1.
\begin{lemma}
\label{lemma:gamma-function}
For any real $a > 0$, the function $f(x) = \Gamma(a+x) \Gamma(a-x)$ satisfies
$$
\forall x \in [0, a) \qquad x \cdot f''(x) \ge f'(x) \; .
$$
\end{lemma}
\begin{proof}
First note that $f(x)$ is even, i.e., $f(x) = f(-x)$. Hence, odd terms of its
Maclaurin series are zero. The property $x \cdot f''(x) \ge f'(x)$ easily
follows if we show that the coefficients of the Maclaurin expansion of $f(x)$
around $0$ are non-negative, except possibly for the zero-order term $a_0$.
Indeed, if
$$
f(x) = \sum_{n=0}^\infty a_{2n} x^{2n} \qquad \text{and} \qquad a_2, a_4, a_6, \dots \ge 0
$$
then the condition $x \cdot f''(x) \ge f'(x)$ is equivalent to
$$
x \sum_{n=0}^\infty (2n)(2n-1) \cdot a_{2n} x^{2n-2} \ge \sum_{n=0}^\infty (2n) \cdot a_{2n} x^{2n-1} \; ,
$$
which holds for any $x \ge 0$ since each term on the right-hand side,
$(2n)(2n-1) a_{2n} x^{2n-1}$, is bigger or equal to the corresponding term on
the left-hand side, $(2n) a_{2n} x^{2n-1}$.

Since $\Gamma(x)$ is positive for any real $x > 0$, the function $f(x)$ is
positive on $(-a,a)$ and hence we can take its logarithm $g(x) = \ln(f(x))$.
Note that if $g(x)$ has non-negative (even) coefficients of its Maclaurin
expansion, except for possibly for the zero-order term, then the same holds for
$f(x) = \exp(g(x))$ since the coefficients of Maclaurin expansion $\exp(z) =
\sum_{n=0}^n \frac{z^n}{n!}$ are positive.

It thus remains to show that the coefficients of the Maclaurin expansion of
$g(x) = \ln(\Gamma(a+x) \Gamma(a-x))$ are non-negative. These coefficients can
be expressed in terms of \emph{logarithmic derivatives} of the Gamma function,
also called \emph{polygamma functions}. These are defined for any $n \ge 0$ as
any complex $x \in \C \setminus \N_{0}$ as
$$
\psi^{(n)}(x) = \frac{d^n\ln(\Gamma(x))}{dx^n} \; .
$$
The Mclaurin of expansion of $g(x)$ is
$$
g(x)
= \ln \left( \Gamma(a+x) \Gamma(a-x) \right)
= 2 \ln(\Gamma(a)) + \sum_{\substack{n \ge 2 \\ \text{$n$ even}}} \frac{\psi^{(n-1)}(a) \cdot x^n}{2^{n-1} n!} \; .
$$
The fact that the coefficients $\psi^{(n-1)}(a)/n!$ are non-negative for even $n
\ge 2$ can be easily seen from the integral representation of polygamma
functions,
$$
\psi^{(n)}(z) = (-1)^{n+1} \int_0^\infty \frac{t^n e^{-zt}}{1-e^{-t}} dt \; ,
$$
valid for any $n \ge 1$ and any complex $z$ such that $\Re(z) > 0$.
\end{proof}

\begin{proof}[Proof of Theorem~\ref{theo:kt_is_mba}]
The first assumption is verified because
\[
\epsilon \frac{\Gamma(1/2) \cdot \Gamma(1/2)}{\pi \cdot 0!} = \epsilon ,
\]
which holds true since $\Gamma(1/2) = \sqrt{\pi}$.

The second assumption is verified by Lemma~\ref{lemma:gamma-function}.

For the third assumption we have to verify that, for $z_t \in [-1,1]$, the following holds
\[
(1+\frac{x_{t-1}}{t} z_t) \frac{2^{t-1} \cdot \Gamma \left(\frac{t}{2} + \frac{x_{t-1}}{2} \right) \cdot \Gamma \left(\frac{t}{2} - \frac{x_{t-1}}{2} \right)}{\pi \cdot (t-1)!}
- \frac{2^t \cdot \Gamma \left(\frac{t+1}{2} + \frac{x_{t-1}+z_t}{2} \right) \cdot \Gamma \left(\frac{t+1}{2} - \frac{x_{t-1}+z_t}{2} \right)}{\pi \cdot t!} \geq 0\;.
\]
Given that the function $f$ does not depend on the past $z_t$, using Lemma~\ref{lemma:gamma-function}, it is enough to prove it for $z_t \in \{-1,1\}$.
First, consider the case that $z_t=1$. We have
\begin{align*}
\frac{2^t \cdot \Gamma \left(\frac{t+1}{2} + \frac{x_{t-1}+z_t}{2} \right) \cdot \Gamma \left(\frac{t+1}{2} - \frac{x_{t-1}+z_t}{2} \right)}{\pi \cdot t!}
&= \frac{2^t \cdot \Gamma \left(1+\frac{t}{2} + \frac{x_{t-1}}{2} \right) \cdot \Gamma \left(\frac{t}{2} - \frac{x_{t-1}}{2} \right)}{\pi \cdot t!} \\
&= \frac{2^t \cdot \left(\frac{t}{2} + \frac{x_{t-1}}{2} \right) \Gamma \left(\frac{t}{2} + \frac{x_{t-1}}{2} \right) \cdot \Gamma \left(\frac{t}{2} - \frac{x_{t-1}}{2} \right)}{\pi \cdot t!} \\
&= \left(1 + \frac{x_{t-1}}{t} \right) \frac{2^{t-1} \Gamma \left(\frac{t}{2} + \frac{x_{t-1}}{2} \right) \cdot \Gamma \left(\frac{t}{2} - \frac{x_{t-1}}{2} \right)}{\pi \cdot (t-1)!} \; .
\end{align*}
where in the second equality we used $\Gamma(x+1) = x \cdot \Gamma(x)$ which holds for any
real $x > 0$. In the same way, when $z=-1$ we have
\begin{align*}
\frac{2^t \cdot \Gamma \left(\frac{t+1}{2} + \frac{x_{t-1}+z_t}{2} \right) \cdot \Gamma \left(\frac{t+1}{2} - \frac{x_{t-1}+z_t}{2} \right)}{\pi \cdot t!}
&= \frac{2^t \cdot \Gamma \left(\frac{t}{2} + \frac{x_{t-1}}{2} \right) \cdot \Gamma \left(1+\frac{t}{2} - \frac{x_{t-1}}{2} \right)}{\pi \cdot t!} \\
&= \frac{2^t \cdot \Gamma \left(\frac{t}{2} + \frac{x_{t-1}}{2} \right) \cdot \left(\frac{t}{2} - \frac{x_{t-1}}{2} \right) \Gamma \left(\frac{t}{2} - \frac{x_{t-1}}{2} \right)}{\pi \cdot t!} \\
&= \left(1 - \frac{x_{t-1}}{t} \right) \frac{2^{t-1} \Gamma \left(\frac{t}{2} + \frac{x_{t-1}}{2} \right) \cdot \Gamma \left(\frac{t}{2} - \frac{x_{t-1}}{2} \right)}{\pi \cdot (t-1)!} \; .
\end{align*}
\end{proof}


\subsection{Proof of Corollary~\ref{cor:kt_hilbert}}

First we state some technical lemmas that will be used in the following proofs.

Define the Lambert function $W(x):\R\rightarrow\R$ as the one that satisfies the equality\footnote{For $x<0$ the Lambert function is multivalued. Hence, to avoid complication and because we only need positive arguments, we will define it only for positive values of $x$.}
\begin{equation}
\label{eq:lambert}
x=W(x) \exp \left(W(x)\right), \ \forall x\geq0.
\end{equation}
It satisfies the following properties.
%
\begin{lemma}
\label{lemma:lambert}
The Lambert function satisfies $0.6321 \log(x+1) \leq W(x) \leq \log(x+1), \forall x\geq0$.
\end{lemma}
%
\begin{proof}
We first prove the lower bound. From \eqref{eq:lambert} we have
\begin{align}
W(x) &= \log\left(\frac{x}{W(x)}\right) \label{eq:lm_lambert_1} \\
&= \log\left(\frac{x}{\log(x/W(x))}\right). \label{eq:lm_lambert_1b}
\end{align}
From the first equality, for any $a>0$, we get
\[
W(x) \leq \frac{1}{a\, e}\left(\frac{x}{W(x)}\right)^a
\]
that is
\begin{equation}
\label{eq:lm_lambert_2}
W(x) \leq \left(\frac{1}{a\, e}\right)^\frac{1}{1+a} x^\frac{a}{1+a}.
\end{equation}
Using \eqref{eq:lm_lambert_2} in \eqref{eq:lm_lambert_1}, we have
\begin{align*}
W(x) 
\geq \log\left(\frac{x}{\left(\frac{1}{a\, e}\right)^\frac{1}{1+a} x^\frac{a}{1+a}}\right) 
= \frac{1}{1+a}\log\left(a \, e\, x\right)~.
\end{align*}
Consider now the function $g(x)=\frac{x}{x+1} - \frac{b}{\log(1+b) (b+1)} \log(x+1), x\geq b$. This function has a maximum in $x^*=(1+\frac{1}{b}) \log(1+b)-1$, the derivative is positive in $[0,x^*]$ and negative in $[x^*,b]$. Hence the minimum is in $x=0$ and in $x=b$, where it is equal to $0$.
Using the property just proved on $g$, we have that for $x\leq b$, setting $a=\frac{1}{x}$, we have
\begin{align*}
W(x) 
\geq \frac{x}{x+1} \geq \frac{b}{\log(1+b) (b+1)} \log(x+1)~.
\end{align*}
For $x>b$, setting $a=\frac{x+1}{e x}$, we have
\begin{align}
W(x) 
&\geq \frac{e\,x}{(e+1) x + 1} \log(x+1) \geq \frac{e\,b}{(e+1) b + 1} \log(x+1)
\end{align}
Hence, we set $b$ such that 
\[
\frac{e\, b}{(e+1)b + 1} = \frac{b}{\log(1+b) (b+1)}
\]
Numberically, $b=1.71825...$, so
\[
W(x) \geq 0.6321 \log(x+1)~. \qedhere
\]

For the upper bound, we use Theorem~2.3 in \cite{hoorfar2008inequalities}, that says that
\[
W(x) \leq \log\frac{x+C}{1+\log(C)}, \quad \forall x> -\frac{1}{e}, \ C>\frac{1}{e}.
\]
Setting $C=1$, we obtain the stated bound.
\end{proof}

\begin{lemma}
Define $f(\theta)= \beta \exp\frac{x^2}{2 \alpha}$, for $\alpha,\beta>0$, $x\geq0$. Then
\[
f^*(y)=y \sqrt{\alpha W\left(\frac{\alpha y^2}{\beta^2}\right)} - \beta \exp\left(\frac{W\left(\frac{\alpha y^2}{\beta^2}\right)}{2}\right).
\]
Moreover
\[
f^*(y) \leq y \sqrt{\alpha \log \left(\frac{\alpha y^2}{\beta^2} +1 \right)} - \beta.
\]
\end{lemma}
\begin{proof}
From the definition of Fenchel dual, we have
\begin{align*}
f^*(y)= \max_{x} \  x\, y - f(x) = \max_{x} \  x\, y - \beta \exp\frac{x^2}{2 \alpha} \leq x^*\,y -\beta
\end{align*}
where $x^*= \argmax_{x} x\, y - f(x)$. We now use the fact that $x^*$ satisfies $y = f'(x^*)$, to have
\begin{align*}
x^*=\sqrt{\alpha W\left(\frac{\alpha y^2}{\beta^2}\right)},
\end{align*}
where the $W(\cdot)$ is the Lambert function.
Using Lemma~\ref{lemma:lambert}, we obtain the stated bound.
\end{proof}

\begin{lemma}[ {\citep[Example 13.7]{BauschkeC2011}} ]
Let $\phi:\R \rightarrow (-\infty, +\infty]$ be even. Then $(\phi \ast \norm{\cdot})^*=\phi^* \ast \norm{\cdot}$.
\end{lemma}

\begin{cor}
\label{cor:dual_exp_square}
Define $f(\theta)= \beta \exp\frac{\norm{\theta}^2}{2 \alpha}$, for $\alpha,\beta>0$. Then
\[
f^*(w) \leq  \norm{w} \sqrt{\alpha \log \left(\frac{\alpha \norm{w}^2}{\beta^2} +1 \right)} - \beta.
\]
\end{cor}

We also state a some useful identities and inequalities.
For any $ 0\leq p < 1$
\[
D\left(\frac{1}{2}+\frac{p}{2}\middle\|\frac{1}{2}\right) = D\left(\frac{1}{2}-\frac{p}{2}\middle\|\frac{1}{2}\right)= \frac{1+p}{2} \log(1+p) + \frac{1-p}{2} \log(1-p).
\]
The extension for continuity of $D(\frac{1}{2}+\frac{p}{2}||\frac{1}{2})$ in $p=1$ is $\log(2)$.
Also,
\[
\left(\frac{n}{n-q}\right)^{n-q} \left(\frac{n}{q}\right)^{q} = 2^n \exp\left(-n D\left(\frac{q}{n}\middle\|\frac{1}{2}\right)\right)=\exp\left(n\, H\left(\frac{q}{n}\right)\right),
\]
and
\begin{equation}
\label{eq:div_2}
\left(1+x\right)^\frac{1+x}{2} \left(1-x\right)^\frac{1-x}{2}= \exp\left( D\left(\frac{1}{2}+\frac{x}{2}\middle\|\frac{1}{2}\right) \right)
\end{equation}

Also, for any $-\frac{1}{2} \leq x\leq \frac{1}{2}$ we have
\[
\frac{x^2}{2} +\frac{x^4}{12}\leq D\left(\frac{1}{2}+\frac{x}{2}\middle\|\frac{1}{2}\right) \leq \frac{x^2}{2} + \frac{x^4}{5}.
\]

We can now prove Corollary~\ref{cor:kt_hilbert}.
\begin{proof}[Proof of Corollary~\ref{cor:kt_hilbert}]
Using Theorem~\ref{theo:hilbert_reward}, we should compute the Fenchel conjugate function of \eqref{eq:kt_potential}.
However, given the complexity of the function, we prefer first lower bound the wealth at time $T$ and then compute the Fenchel conjugate.
Using Lemma~\ref{lemma:approx_gamma}, we have
\begin{align*}
\ln \frac{\Gamma(a + 1/2) \cdot \Gamma(b + 1/2)}{\pi \cdot T!} 
&\geq -\ln(2) -\frac{1}{2} \ln(T) +\ln \left(\left( \frac{a}{T} \right)^a \left( \frac{b}{T} \right)^b\right) \\
&= -\ln(2) -\frac{1}{2} \ln(T) -T \ln (2) + T \, \DKL\left(\frac{b}{T}\middle\|\frac{1}{2}\right) \; .
\end{align*}
Hence, we have
\begin{align*}
\epsilon  \frac{2^T \cdot \Gamma \left(\frac{T+1}{2} + \frac{1}{2} x_{T} \right) \cdot \Gamma \left(\frac{T+1}{2} - \frac{1}{2} x_{T} \right)}{\pi \cdot T!} 
&\geq \epsilon \exp\left(-\ln(2) -\frac{1}{2} \ln(T) + T \, \DKL\left(\frac{1}{2}+\frac{x_T}{2T}\middle\|\frac{1}{2}\right)\right) \\
&= \frac{\epsilon}{2 \sqrt{T}} \exp\left(T \, \DKL\left(\frac{1}{2}+\frac{x_T}{2T}\middle\|\frac{1}{2}\right)\right) \\
&\geq \frac{\epsilon}{2 \sqrt{T}} \exp\left(\frac{x_T^2}{2T}\right)
\end{align*}
Using Corollary~\ref{cor:dual_exp_square}, we have that the Fenchel conjugate of $f(\theta)=\frac{\epsilon}{2 \sqrt{T}} \exp\left(\frac{\norm{\theta}^2}{2T}\right)$ is upper bounded by $f^*(w) \leq \norm{w} \sqrt{T \log \left(\frac{4 T^2 \norm{w}^2}{\epsilon^2} +1 \right)} +\epsilon(1-\frac{1}{2\sqrt{T}})$.
\end{proof}
