\appendix
\section{Appendix}

\subsection{Proof of Theorem~\ref{theo:hilbert_reward}}

The following Lemma is from~\cite{McMahanO14} and reported here with our notation for completeness.
\begin{lemma}[Extremes]
\label{lemma:extremes}
Let $h:(-a,a) \to \R$ be an even concave twice-differentiable function that
satisfies $x \cdot h''(x) \le h'(x)$ for all $x \in [0,a)$. Then, if vectors
$u,v \in \H$ satisfy $\|u\| + \|v\| < a$, then
\begin{equation}
\label{equation:lemma-extremes-1}
\langle u, v \rangle + h(\|u + v\|) \ge \min \left\{ \|u\| \cdot \|v\| + h(\|v\| + \|u\|), \ - \|u\| \cdot \|v\| + h(\|u\| - \|v\|) \right\} \; .
\end{equation}
Futhermore, for any $b \in [0,a]$ and any $u, v \in \H$ such that $\|v\| < a - b$ and $\|u\| \le b$,
\begin{equation}
\label{equation:lemma-extremes-2}
\langle u, v \rangle + h(\|u + v\|) \ge \min \left\{ b \cdot \|v\| + h(\|v\| + b), \ - b \cdot \|v\| + h(\|u\| - b) \right\} \; .
\end{equation}
\end{lemma}
\begin{proof}[Proof of Lemma \ref{lemma:extremes}]
If $u$ or $v$ is zero, the inequality \eqref{equation:lemma-extremes-1} clearly holds. From now on we assume that
$u,v$ are non-zero. Let $c$ be the cosine of the angle of between $u$ and $v$.
More formally,
$$
c = \frac{\langle u, v \rangle}{\|u\| \cdot \|v\|} \; .
$$
With this notation, the left-hand side is
$$
\langle u, v \rangle + h(\|u + v\|) = c \|u\| \cdot \|v\|  + h(\sqrt{\|u\|^2 + \|v\|^2 + 2 c \|u\| \cdot \|v\|}) \; .
$$
We consider the last expression as a function of $c$ and we call it $f(c)$. The
inequality \eqref{equation:lemma-extremes-1} is equivalent to
$$
\forall c \in [-1,1] \qquad \qquad f(c) \ge \min \left\{f(+1), f(-1)\right\} \; .
$$
The last inequality is clearly true if $f:[-1,1] \to \R$ is concave. We now
check that $f$ is indeed concave, which we prove by showing that the second
derivative is non-positive. The first derivate of $f$ is
$$
f'(c) = \|u\| \cdot \|v\| + \frac{h'(\sqrt{\|u\|^2 + \|v\|^2 + 2 c \|u\| \cdot \|v\|}) \cdot \|u\| \cdot \|v\|}{\sqrt{\|u\|^2 + \|v\|^2 + 2 c \|u\| \cdot \|v\|}} \; .
$$
The second derivative of $f$ is
$$
f''(c) = \|u\|^2 \cdot \|v\|^2 \cdot \frac{h''(\sqrt{\|u\|^2 + \|v\|^2 + 2 c \|u\| \cdot \|v\|})  - \frac{1}{\sqrt{\|u\|^2 + \|v\|^2 + 2c \|u\| \cdot \|v\|}} \cdot h'(\sqrt{\|u\|^2 + \|v\|^2 + 2 c \|u\| \cdot \|v\|})  }{\|u\|^2 + \|v\|^2 + 2 c \|u\| \cdot \|v\|} \; .
$$
If we consider $x=\sqrt{\|u\|^2 + \|v\|^2 + 2 c \|u\| \cdot \|v\|}$, the
assumption $x \cdot h''(x) \le h'(x)$ implies that $f''(c)$ is non-positive.
This finishes the proof of the inequality \eqref{equation:lemma-extremes-1}.

Inequality \eqref{equation:lemma-extremes-2}, can be derived from inequality
\eqref{equation:lemma-extremes-1} as follows
\begin{align*}
\langle u, v \rangle + h(\|u + v\|)
& \ge \min_{u \in \H : \|u\| \le b} \min \left\{ \|u\| \cdot \|v\| + h(\|v\| + b), \ - \|u\| \cdot \|v\| + h(\|u\| - \|v\|) \right\} \\
& = \min_{z \in [0,b]} \min \left\{ z \cdot \|v\| + h(\|v\| + z), \ - z \cdot \|v\| + h(\|u\| - z) \right\} \\
& = \min_{z \in [-b,b]} z \cdot \|v\| + h(\|v\| + z) \\
& = \min \left\{ b \cdot \|v\| + h(\|v\| + b), \ - b \cdot \|v\| + h(\|u\| - b) \right\} \; .
\end{align*}
The inequality in the chain is the inequality \eqref{equation:lemma-extremes-1}.
The last equality follows since $g(z) = z \cdot \|v\| + h(\|v\| + z)$
is concave.
\end{proof}

We will also use the following Theorem from~\citet{McMahanO14}, that allows to recast the problem of proving an upper bound to the regret to the one of proving a lower bound to the \emph{wealth} of the algorithm.
The reward and regret view on online learning are equivalent: an algorithm guarantees low regret iff it guarantees high reward. The following Theorem makes this claim rigorous. Notice that the algorithm is exactly the same in the two setting.
\begin{theorem}[\citet{McMahanO14}]
  \label{thm:rrdual}
  Let $\Psi:\mathcal{H} \rightarrow (-\infty, +\infty]$ be a lower semicontinuos and convex function, with $dom \Psi \neq \emptyset$. An
  algorithm for the player guarantees
  \[
  \gain_n \geq \Psi\left(\sum_{t=1}^n g_t\right) - \epsilon \quad \quad \quad \textnormal{ for any } g_1, \dots, g_n
  \]
  for a constant $\epsilon \in \R$ if and only if it
  guarantees
  \begin{equation}\label{eq:regb}
  \qquad Regret_n(u) \leq \Psi^*(u) + \epsilon \quad \quad \quad \textnormal{ for all } u \in \fH \textnormal{ and } g_t \in \fH, \forall 1\leq t\leq n~.
  \end{equation}
\end{theorem}
The theorem implies that a betting algorithm can be used for online learning and vice-versa. However, as it was already stressed in \citet{McMahanO14}, the reward view has the big advantage of having one variable less, the competitor $u$.
Moreover, as we will show in the following, designing and analyzing algorithms in one of the two views could be much easier than in the other one.

\begin{proof}[Proof of Theorem~\ref{theo:hilbert_reward}]
  We will first prove a lower bound on the Wealth of the algorithm, that derive a regret bound using Theorem~\ref{thm:rrdual}.
  For simplicity denote by $f_t(\cdot)=f\left(\cdot, \{\norm{g_1}, \ldots, \norm{g_t}\}\right)$.
  We will prove the thesis by induction. The base case is verified from the first point of Assumption~\ref{assumption:1-d_algo}. The, we assume that 
  \[
  \epsilon + \sum_{t=1}^{n-1} \langle g_t, w_t \rangle \geq f_{n-1}\left( \norm{\sum_{t=1}^{n-1} g_t}\right),
  \]
  and we want to prove that 
  \[
  \epsilon + \sum_{t=1}^{n} \langle g_t, w_t \rangle \geq f_{n}\left( \norm{\sum_{t=1}^{n} g_t}\right)~.
  \]
  We have that
  \begin{align*}
  \epsilon + \sum_{t=1}^{n} &\langle g_t, w_t \rangle - f_n\left( \norm{\sum_{t=1}^{n} g_t}\right) \\
  &= \langle g_n, w_n \rangle + \epsilon + \sum_{t=1}^{n-1} \langle g_t, w_t \rangle - f_n\left( \norm{\sum_{t=1}^{n} g_t}\right)\\
  &= \left(1+\frac{b_n}{\norm{\theta_{n-1}}}\langle \theta_{n-1},g_n \rangle \right)\left(\sum_{t=1}^{n-1} \langle g_t, w_t \rangle +\epsilon \right) - f_n\left( \norm{\sum_{t=1}^{n} g_t}\right)\\
  &\geq \left(1+\frac{b_n}{\norm{\theta_{n-1}}}\langle \theta_{n-1},g_n \rangle \right) f_{n-1}\left( \norm{\sum_{t=1}^{n-1} g_t}\right) - f_n\left( \norm{\sum_{t=1}^{n} g_t}\right)\\
  &= \left(1+\frac{b_n}{\norm{\theta_{n-1}}}\langle \theta_{n-1},g_n \rangle \right) f_{n-1}\left( \norm{\sum_{t=1}^{n-1} g_t}\right) - f_n\left( \norm{g_n + \sum_{t=1}^{n-1} g_t}\right)\\
  %&= \left(1+\frac{b_n}{\norm{\theta_{n-1}}}\langle \theta_{n-1},g_n \rangle \right) f_{n-1}\left( \norm{\theta_{n-1}}\right) - f_n\left( \sqrt{\norm{\theta_{n-1}}^2 + \norm{g_n}^2 + 2 \langle \theta_{n-1}, g_n \rangle} \right)\\
  &\geq \min_{r\in \{-1,1\}} \left(1+ r\, b_n \norm{g_n} \right) f_{n-1}\left( \norm{\theta_{n-1}}\right) - f_n\left(\left| \norm{\theta_{n-1}} + r \norm{g_n}\right|\right)\\
  &= \min_{r\in \{-1,1\}} \left(1+ r\, b_n \norm{g_n} \right) f_{n-1}\left( \norm{\theta_{n-1}}\right) - f_n\left( \norm{\theta_{n-1}} + r \norm{g_n}\right)\\
  &\geq 0,
  \end{align*}
  where the first inequality comes from the induction hypothesis, the second one using Lemma~\ref{lemma:extremes} and the last one by the hypothesis on the \ac{MBA}.
  
  An application of Theorem~\ref{thm:rrdual} finishes the proof.
\end{proof}

\section{Proof of of Lemma~\ref{lemma:kt} and Lemma Theorem~\ref{theo:logloss}}

\begin{proof}[Proof of Lemma~\ref{lemma:kt}]
We prove the equality by induction $T$. For $T=0$ the equality is
$$
0 = - \ln \left( \frac{\Gamma(1/2) \cdot \Gamma(1/2)}{\pi \cdot 0!} \right)
$$
which holds true since $\Gamma(1/2) = \sqrt{\pi}$.
For $T \ge 1$, we use the induction hypothesis for $T-1$ and some algebraic manipulation
\begin{align*}
\sum_{t=1}^T \ell(p_t, q_t)
& = \ell(p_T, q_T) + \sum_{t=1}^{T-1} \ell(p_t, q_t) \\
& = \ell(p_T, q_T) - \ln \left( \frac{\Gamma(a_{T-1} + 1/2) \cdot \Gamma(b_{T-1} + 1/2)}{\pi \cdot (T-1)!} \right) \\
& = q_T \ln\left( \frac{1}{p_T}\right) + (1-q_T) \ln  \left( \frac{1}{1 - p_T} \right) - \ln \left( \frac{\Gamma(a_{T-1} + 1/2) \cdot \Gamma(b_{T-1} + 1/2)}{\pi \cdot (T-1)!} \right) \\
& = - q_T \ln\left( \frac{\frac{1}{2} + a_{T-1}}{T} \right) - (1-q_T) \ln\left( \frac{\frac{1}{2} + b_{T-1}}{T} \right) - \ln \left( \frac{\Gamma(a_{T-1} + 1/2) \cdot \Gamma(b_{T-1} + 1/2)}{\pi \cdot (T-1)!} \right) \\
& = - \ln\left( \left( \frac{\frac{1}{2} + a_{T-1}}{T} \right)^{q_T} \left( \frac{\frac{1}{2} + b_{T-1}}{T} \right)^{1-q_T} \frac{\Gamma(a_{T-1} + 1/2) \cdot \Gamma(b_{T-1} + 1/2)}{\pi \cdot (T-1)!} \right) \\
& = - \ln\left( \left( \frac{1}{2} + a_{T-1} \right)^{q_T} \left( \frac{1}{2} + b_{T-1} \right)^{1-q_T} \frac{\Gamma(a_{T-1} + 1/2) \cdot \Gamma(b_{T-1} + 1/2)}{\pi \cdot T!}  \right) \; .
\end{align*}
It remains to prove that expression inside logarithm equals
$$
\frac{\Gamma(a_{T} + 1/2) \cdot \Gamma(b_T + 1/2)}{\pi \cdot T!}
$$
We consider two cases. If $q_T = 1$ then
\begin{align*}
& \left( \frac{1}{2} + a_{T-1} \right)^{q_T} \left( \frac{1}{2} + b_{T-1} \right)^{1-q_T} \frac{\Gamma(a_{T-1} + 1/2) \cdot \Gamma(b_{T-1} + 1/2)}{\pi \cdot T!} \\
& = \left( \frac{1}{2} + a_{T-1} \right) \frac{\Gamma(a_{T-1} + 1/2) \cdot \Gamma(b_{T-1} + 1/2)}{\pi \cdot T!} \\
& = \frac{\Gamma(a_{T-1} + 3/2) \cdot \Gamma(b_{T-1} + 1/2)}{\pi \cdot T!} \\
& = \frac{\Gamma(a_{T} + 1/2) \cdot \Gamma(b_T + 1/2)}{\pi \cdot T!} \; ,
\end{align*}
where we have used that $\Gamma(x+1) = x \Gamma(x)$ for any real $x > 0$ and that $a_T = a_{T-1} + q_T = a_{T-1} + 1$ and $b_T = b_{T-1} + (1-q_T) = b_{T-1}$.
Similarly, if $q_T = 0$ then
\begin{align*}
& \left( \frac{1}{2} + a_{T-1} \right)^{q_T} \left( \frac{1}{2} + b_{T-1} \right)^{1-q_T} \frac{\Gamma(a_{T-1} + 1/2) \cdot \Gamma(b_{T-1} + 1/2)}{\pi \cdot T!} \\
& = \left( \frac{1}{2} + b_{T-1} \right) \frac{\Gamma(a_{T-1} + 1/2) \cdot \Gamma(b_{T-1} + 1/2)}{\pi \cdot T!} \\
& = \frac{\Gamma(a_{T-1} + 1/2) \cdot \Gamma(b_{T-1} + 3/2)}{\pi \cdot T!} \\
& = \frac{\Gamma(a_T + 1/2) \cdot \Gamma(b_T + 3/2)}{\pi \cdot T!} \; ,
\end{align*}
where we have used that $\Gamma(x+1) = x \Gamma(x)$ for any real $x > 0$ and that $a_T = a_{T-1} + q_T = a_{T-1}$ and $b_T = b_{T-1} + (1-q_T) = b_{T-1} + 1$.
\end{proof}

\begin{proof}[Proof of Theorem~\ref{theo:logloss}]
The formulas for $a_T, b_T$ and $p^*$ remain exactly the same as in Section~\ref{section:online-data-compression}:
\begin{align*}
a_T & = \sum_{t=1}^T q_t \; , \\
b_T & = \sum_{t=1}^T (1 - q_t) = T - a_T \; , \\
p^* & =  \argmin_{p \in [0,1]} \sum_{t=1}^T \ell(p, q_t) = \frac{a_T}{a_T + b_T} = \frac{a_T}{T} = \frac{\sum_{t=1}^T q_t}{T} \; .
\end{align*}
Hence, we have $p_t = \frac{\frac{1}{2} + \sum_{t=1}^{t-1} q_i}{t} = \frac{\frac{1}{2} + a_{t-1}}{t}$.

Let $\widetilde q_1, \widetilde q_2, \dots, \widetilde q_T$ independent
Bernoulli variables with parameters $q_1, q_2, \dots, q_T$ respectively.
Let
\begin{align*}
\widetilde a_t & = \sum_{i=1}^t \widetilde q_i  \; , \\
\widetilde b_t & = t - \widetilde a_t \; , \\
\widetilde p_t & = \frac{\frac{1}{2} + \sum_{i=1}^{t-1} \widetilde q_i}{t} = \frac{\frac{1}{2} + \widetilde a_{t-1}}{t} \; , \\
\widehat p & = \frac{\widetilde a_T}{T} = \frac{\widetilde a_T}{\widetilde a_T + \widetilde b_T} \; . \\
\end{align*}
Clearly,
\begin{align*}
q_i & = \Exp[\widetilde q_i] \; , \\
a_t & = \Exp[\widetilde a_t] \; , \\
b_t & = \Exp[\widetilde b_t] \; , \\
p_t & = \Exp[\widetilde p_t] \; , \\
p^* & = \Exp[\widehat p] \; .
\end{align*}

Since $\ell(p,q)$ is linear in $q$ and convex in $p$, we have
\begin{align*}
\sum_{t=1}^T \ell(p_t, q_t)
& = \sum_{t=1}^T \ell( \Exp[ \widetilde p_t], \Exp[\widetilde q_t]) \\
& = \Exp\left[ \sum_{t=1}^T \ell( \Exp[ \widetilde p_t], \widetilde q_t) \right] \\
& \le \Exp\left[ \sum_{t=1}^T \ell( \widetilde p_t, \widetilde q_t) \right] \\
& \le - \Exp\left[ \ln \left\{ \frac{1}{2\sqrt{T}} \left( \frac{\widetilde a_T}{T} \right)^{\widetilde a_T} \left( \frac{\widetilde b_T}{T} \right)^{\widetilde b_T} \right\} \right] \\
& = \ln(2) + \frac{1}{2} \ln(T) + \Exp \left[ - \widetilde a_T \ln \left( \frac{\widetilde a_T}{T} \right) - \widetilde b_T \ln \left( \frac{\widetilde b_T}{T} \right) \right] \\
& = \ln(2) + \frac{1}{2} \ln(T) + T \cdot \Exp \left[ H(\widehat p) \right] \\
& \le \ln(2) + \frac{1}{2} \ln(T) + T \cdot H(\Exp[\widehat p]) \\
& = \ln(2) + \frac{1}{2} \ln(T) + T \cdot H(p^*)~.
\end{align*}
\end{proof}

\section{Proof of Lemma \ref{lemma:gamma-function}}

\begin{proof}
First note that $f(x)$ is even, i.e., $f(x) = f(-x)$. Hence, odd terms of its
Maclaurin series are zero. The property $x \cdot f''(x) \ge f'(x)$ easily
follows if we show that the coefficients of the Maclaurin expansion of $f(x)$
around $0$ are non-negative, except possibly for the zero-order term $a_0$.
Indeed, if
$$
f(x) = \sum_{n=0}^\infty a_{2n} x^{2n} \qquad \text{and} \qquad a_2, a_4, a_6, \dots \ge 0
$$
then the condition $x \cdot f''(x) \ge f'(x)$ is equivalent to
$$
x \sum_{n=0}^\infty (2n)(2n-1) \cdot a_{2n} x^{2n-2} \ge \sum_{n=0}^\infty (2n) \cdot a_{2n} x^{2n-1} \; ,
$$
which holds for any $x \ge 0$ since each term on the right-hand side,
$(2n)(2n-1) a_{2n} x^{2n-1}$, is bigger or equal to the corresponding term on
the left-hand side, $(2n) a_{2n} x^{2n-1}$.

Since $\Gamma(x)$ is positive for any real $x > 0$, the function $f(x)$ is
positive on $(-a,a)$ and hence we can take its logarithm $g(x) = \ln(f(x))$.
Note that if $g(x)$ has non-negative (even) coefficients of its Maclaurin
expansion, except for possibly for the zero-order term, then the same holds for
$f(x) = \exp(g(x))$ since the coefficients of Maclaurin expansion $\exp(z) =
\sum_{n=0}^n \frac{z^n}{n!}$ are positive.

It thus remains to show that the coefficients of the Maclaurin expansion of
$g(x) = \ln(\Gamma(a+x) \Gamma(a-x))$ are non-negative. These coefficients can
be expressed in terms of \emph{logarithmic derivatives} of the Gamma function,
also called \emph{polygamma functions}. These are defined for any $n \ge 0$ as
any complex $x \in \C \setminus \N_{0}$ as
$$
\psi^{(n)}(x) = \frac{d^n\ln(\Gamma(x))}{dx^n} \; .
$$
The Mclaurin of expansion of $g(x)$ is
$$
g(x)
= \ln \left( \Gamma(a+x) \Gamma(a-x) \right)
= 2 \ln(\Gamma(a)) + \sum_{\substack{n \ge 2 \\ \text{$n$ even}}} \frac{\psi^{(n-1)}(a) \cdot x^n}{n!} \; .
$$
The fact that the coefficients $\psi^{(n-1)}(a)/n!$ are non-negative for even $n
\ge 2$ can be easily seen from the integral representation of polygamma
functions,
$$
\psi^{(n)}(z) = (-1)^{n+1} \int_0^\infty \frac{t^n e^{-zt}}{1-e^{-t}} dt \; ,
$$
valid for any $n \ge 1$ and any complex $z$ such that $\Re(z) > 0$.
\end{proof}


\subsection{KL properties}
We first state a couple of useful indentities.
For any $ 0\leq p < 1$
\[
D\left(\frac{1}{2}+\frac{p}{2}\middle\|\frac{1}{2}\right) = D\left(\frac{1}{2}-\frac{p}{2}\middle\|\frac{1}{2}\right)= \frac{1+p}{2} \log(1+p) + \frac{1-p}{2} \log(1-p).
\]
The extension for continuity of $D(\frac{1}{2}+\frac{p}{2}||\frac{1}{2})$ in $p=1$ is $\log(2)$.
Also,
\[
\left(\frac{n}{n-q}\right)^{n-q} \left(\frac{n}{q}\right)^{q} = 2^n \exp\left(-n D\left(\frac{q}{n}\middle\|\frac{1}{2}\right)\right)=\exp\left(n\, H\left(\frac{q}{n}\right)\right),
\]
and
\begin{equation}
\label{eq:div_2}
\left(1+x\right)^\frac{1+x}{2} \left(1-x\right)^\frac{1-x}{2}= \exp\left( D\left(\frac{1}{2}+\frac{x}{2}\middle\|\frac{1}{2}\right) \right)
\end{equation}

Also, for any $-\frac{1}{2} \leq x\leq \frac{1}{2}$ we have
\[
\frac{x^2}{2} +\frac{x^4}{12}\leq D\left(\frac{1}{2}+\frac{x}{2}\middle\|\frac{1}{2}\right) \leq \frac{x^2}{2} + \frac{x^4}{5}.
\]