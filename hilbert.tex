\section{Online Linear Optimization over Hilbert Space}
\label{section:online-linear-optimization-hilbert-space}

In this section, we show our first reduction: From continuos coin betting to \ac{OLO} in Hilbert spaces.

Consider an online game where each round we predict $w_t$ in a Hilbert space
$\H$ and the adversary chooses $g_t \in \H$ such that $\|g_t\| \le 1$. The
algorithm tries to maximize its cumulative gain $\sum_{t=1}^T \langle g_t, w_t
\rangle$. The goal of algorithm is to have small regret after $T$ rounds w.r.t.
competitor $u \in \H$:
$$
\Regret_T(u) = \sum_{t=1}^t \langle g_t, u \rangle - \sum_{t=1}^T \langle g_t, w_t \rangle \; .
$$

We will use the following Theorem from~\citet{McMahanO14}, that allows to recast the problem of proving an upper bound to the regret to the one of proving a lower bound to the \emph{wealth} of the algorithm.
The reward and regret view on online learning are equivalent: an algorithm guarantees low regret iff it guarantees high reward. The following Theorem makes this claim rigorous. Notice that the algorithm is exactly the same in the two setting.
\begin{theorem}[\citet{McMahanO14}]
  \label{thm:rrdual}
  Let $\Psi:\mathcal{H} \rightarrow (-\infty, +\infty]$ be a lower semicontinuos and convex function, with $dom \Psi \neq \emptyset$. An
  algorithm for the player guarantees
  \[
  \gain_n \geq \Psi\left(\sum_{t=1}^n g_t\right) - \epsilon \quad \quad \quad \textnormal{ for any } g_1, \dots, g_n
  \]
  for a constant $\epsilon \in \R$ if and only if it
  guarantees
  \begin{equation}\label{eq:regb}
  \qquad Regret_n(u) \leq \Psi^*(u) + \epsilon \quad \quad \quad \textnormal{ for all } u \in \fH \textnormal{ and } g_t \in \fH, \forall 1\leq t\leq n~.
  \end{equation}
\end{theorem}
So, a betting algorithm can be used for online learning and vice-versa. However, as it was already stressed in \citet{McMahanO14}, the reward view has the big advantage of having one variable less, the competitor $u$.
Moreover, as we will show in the following, designing and analyzing algorithms in one of the two views could be much easier than in the other one.










