\documentclass{article}

\usepackage{fullpage,amsthm,amsmath,amssymb}

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\Exp}{\mathbf{E}}
\DeclareMathOperator{\Regret}{Regret}
\DeclareMathOperator{\Wealth}{Wealth}

\newcommand{\N}{\mathbb{N}}  % real numbers
\newcommand{\R}{\mathbb{R}}  % natural numbers
\newcommand{\C}{\mathbb{C}}  % complex numbers

\renewcommand{\H}{\mathcal{H}}
\newcommand{\indicator}{\mathbf{1}}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}

\begin{document}

\title{Data Compression and Online Linear Optimization}
\author{Francesco Orabona \and D\'avid P\'al}

\maketitle

\begin{abstract}
Starting from Krichevsky-Trofimov estimator for estimating
the bias of a coin, we derive an algorithm for online linear
optimization over a Hilbert space.
\end{abstract}

\section{Introduction}

Krichevsky-Trofimov (KT) estimator is a classical estimator of bias of coin.
Given $T$ outcomes of a coin flip $q_1, q_2, \dots, q_T \in \{0,1\}$ where $1$
denotes heads and $0$ tails, KT estimator of coin's probability of heads is
$$
\frac{\frac{1}{2} + \sum_{t=1}^T q_t}{T + 1} \; .
$$
This estimator has many interesting properties. For us, the most crucial one is
that it can be used in online data compression and satisfies a bound on the
number of bits used.

We show that KT-estimator can be used to derive an algorithm for online linear
optimization over a Hilbert space and data compression bound can be used derive
a regret bound for the online linear optimization problem.

First, in section~\ref{section:online-data-compression}, we recap the online
data compression problem and show a bound on number of bits used by the KT
estimator.  Second, in section~\ref{section:log-loss}, we generalize the online
data compression problem to an online prediction with log loss and we show that
a obvious generalization of the KT estimator yields and algorithm with the same
bound. In section~\ref{section:online-linear-optimization-1D}, we explain the
connection with one-dimensional online linear optimization problem.
Finally, in section~\ref{section:online-linear-optimization-hilbert-space},
we generalize it to online linear optimization over a Hilbert space.

\section{Online Data Compression}
\label{section:online-data-compression}

Consider an online data compression problem where we predict a stream of bits;
in each round we predict the next bit based on the previous bits.  More
formally, in each round $t=1,2,\dots$, we predict probability $p_t \in [0,1]$
that the next bit equals one, and then the next bit $q_t \in \{0, 1\}$ is
observed.  Our prediction $p_t$ is scored according to log-loss:
$$
\ell(p_t, q_t) = q_t \ln\left( \frac{1}{p_t}\right) + (1-q_t) \ln  \left( \frac{1}{1 - p_t} \right) \; .
$$
In information theoretic terms, $\ell(p_t, q_t)$ measures the number of bits
needed to encode $q_t$ if we use encoding that uses $\ln(\frac{1}{p_t})$ bits
to encode $+1$ and $\ln(\frac{1}{1 - p_t})$ bits to encode $0$.

After $T$ rounds, the algorithm incurs loss $\sum_{t=1}^T \ell(p_t, q_t)$. We
compare this loss with an offline baseline that in round $1,2,\dots,T$ predicts
the same prediction
$$
p^* = \argmin_{p \in [0,1]} \sum_{t=1}^T \ell(p, q_t) \; .
$$
Note that $p^*$ can be calculated only with the knowledge of all the outcomes
$q_1, q_2, \dots, q_T$; hence the name \emph{offline}.

Let
$$
a_T = \sum_{t=1}^T q_t \qquad \text{and} \qquad b_T = \sum_{t=1}^T (1 - q_t) = T - a_T \; .
$$
We have
\begin{align*}
p^*
= \argmin_{p \in [0,1]} \ a_T \cdot  \ln \left( \frac{1}{p} \right) + b_T \cdot \ln \left( \frac{1}{1-p} \right)
= \argmin_{p \in [0,1]} \ - a_T \cdot  \ln p \ - \  b_T \cdot \ln (1-p)
\end{align*}
Defining $f(p) = - a_T \cdot  \ln p \ - \  b_T \cdot \ln(1-p)$, we can find maximum of
finding point the derivative of $f(p)$ is zero. The derivative of $f(p)$ is
$$
f'(p) = - \frac{a_T}{p} + \frac{b_T}{1-p} \; .
$$
The equation $f'(p) = 0$ has only one solution
$$
p^* = \frac{a_T}{a_T + b_T} = \frac{a_T}{T} = \frac{\sum_{t=1}^T q_t}{T} \; .
$$
Note that $p^*$ differs from the KT estimate $p_T$.

The type of result that we are interested is an upper bound on difference
between the loss of the algorithm and the loss of the baseline
$$
\sum_{t=1}^T \ell(p_t, q_t) - \sum_{t=1}^T \ell(p^*, q_t) \; ,
$$
which can be viewed as the cost for not knowing the fraction of bits set to $1$
ahead of time.

\subsection{KT estimator}

We consider an algorithm that in round $t$ chooses its prediction $p_t$
according to KT estimator
$$
p_t = \frac{\frac{1}{2} + \sum_{t=1}^{t-1} q_i}{t} = \frac{\frac{1}{2} + a_{t-1}}{t} \; .
$$
We show the following equality of loss of the algorithm:

\begin{lemma}[KT Estimator]
For any sequence $q_1, q_2, \dots, q_T \in \{0,1\}$, the log-loss of KT
predictions is
$$
\sum_{t=1}^T \ell(p_t, q_t) =  - \ln \left( \frac{\Gamma(a_T + 1/2) \cdot \Gamma(b_T + 1/2)}{\pi \cdot T!} \right)
$$
where $a_T = \sum_{t=1}^T q_t$ and $b_T = T - a_T$ and $\Gamma(x) =
\int_0^\infty x^{u-1} e^{-x} du$ is the Euler's Gamma function.
\end{lemma}

\begin{proof}
We prove the equality by induction $T$. For $T=0$ the equality is
$$
0 = - \ln \left( \frac{\Gamma(1/2) \cdot \Gamma(1/2)}{\pi \cdot 0!} \right)
$$
which holds true since $\Gamma(1/2) = \sqrt{\pi}$.
For $T \ge 1$, we use the induction hypothesis for $T-1$ and some algebraic manipulation
\begin{align*}
\sum_{t=1}^T \ell(p_t, q_t)
& = \ell(p_T, q_T) + \sum_{t=1}^{T-1} \ell(p_t, q_t) \\
& = \ell(p_T, q_T) - \ln \left( \frac{\Gamma(a_{T-1} + 1/2) \cdot \Gamma(b_{T-1} + 1/2)}{\pi \cdot (T-1)!} \right) \\
& = q_T \ln\left( \frac{1}{p_T}\right) + (1-q_T) \ln  \left( \frac{1}{1 - p_T} \right) - \ln \left( \frac{\Gamma(a_{T-1} + 1/2) \cdot \Gamma(b_{T-1} + 1/2)}{\pi \cdot (T-1)!} \right) \\
& = - q_T \ln\left( \frac{\frac{1}{2} + a_{T-1}}{T} \right) - (1-q_T) \ln\left( \frac{\frac{1}{2} + b_{T-1}}{T} \right) - \ln \left( \frac{\Gamma(a_{T-1} + 1/2) \cdot \Gamma(b_{T-1} + 1/2)}{\pi \cdot (T-1)!} \right) \\
& = - \ln\left( \left( \frac{\frac{1}{2} + a_{T-1}}{T} \right)^{q_T} \left( \frac{\frac{1}{2} + b_{T-1}}{T} \right)^{1-q_T} \frac{\Gamma(a_{T-1} + 1/2) \cdot \Gamma(b_{T-1} + 1/2)}{\pi \cdot (T-1)!} \right) \\
& = - \ln\left( \left( \frac{1}{2} + a_{T-1} \right)^{q_T} \left( \frac{1}{2} + b_{T-1} \right)^{1-q_T} \frac{\Gamma(a_{T-1} + 1/2) \cdot \Gamma(b_{T-1} + 1/2)}{\pi \cdot T!}  \right) \; .
\end{align*}
It remains to prove that expression inside logarithm equals
$$
\frac{\Gamma(a_{T} + 1/2) \cdot \Gamma(b_T + 1/2)}{\pi \cdot T!}
$$
We consider two cases. If $q_T = 1$ then
\begin{align*}
& \left( \frac{1}{2} + a_{T-1} \right)^{q_T} \left( \frac{1}{2} + b_{T-1} \right)^{1-q_T} \frac{\Gamma(a_{T-1} + 1/2) \cdot \Gamma(b_{T-1} + 1/2)}{\pi \cdot T!} \\
& = \left( \frac{1}{2} + a_{T-1} \right) \frac{\Gamma(a_{T-1} + 1/2) \cdot \Gamma(b_{T-1} + 1/2)}{\pi \cdot T!} \\
& = \frac{\Gamma(a_{T-1} + 3/2) \cdot \Gamma(b_{T-1} + 1/2)}{\pi \cdot T!} \\
& = \frac{\Gamma(a_{T} + 1/2) \cdot \Gamma(b_T + 1/2)}{\pi \cdot T!} \; ,
\end{align*}
where we have used that $\Gamma(x+1) = x \Gamma(x)$ for any real $x > 0$ and that $a_T = a_{T-1} + q_T = a_{T-1} + 1$ and $b_T = b_{T-1} + (1-q_T) = b_{T-1}$.
Similarly, if $q_T = 0$ then
\begin{align*}
& \left( \frac{1}{2} + a_{T-1} \right)^{q_T} \left( \frac{1}{2} + b_{T-1} \right)^{1-q_T} \frac{\Gamma(a_{T-1} + 1/2) \cdot \Gamma(b_{T-1} + 1/2)}{\pi \cdot T!} \\
& = \left( \frac{1}{2} + b_{T-1} \right) \frac{\Gamma(a_{T-1} + 1/2) \cdot \Gamma(b_{T-1} + 1/2)}{\pi \cdot T!} \\
& = \frac{\Gamma(a_{T-1} + 1/2) \cdot \Gamma(b_{T-1} + 3/2)}{\pi \cdot T!} \\
& = \frac{\Gamma(a_T + 1/2) \cdot \Gamma(b_T + 3/2)}{\pi \cdot T!} \; ,
\end{align*}
where we have used that $\Gamma(x+1) = x \Gamma(x)$ for any real $x > 0$ and that $a_T = a_{T-1} + q_T = a_{T-1}$ and $b_T = b_{T-1} + (1-q_T) = b_{T-1} + 1$.
\end{proof}

\begin{lemma}
If $T$ is a non-negative integer and $a,b$ are non-negative reals such that $a +
b = T$ then
$$
\frac{\Gamma(a + 1/2) \cdot \Gamma(b + 1/2)}{\pi \cdot T!} \ge \frac{1}{2\sqrt{T}} \left( \frac{a}{T} \right)^a \left( \frac{b}{T} \right)^b \; .
$$
\end{lemma}

Combining the two lemmas we get
\begin{align*}
\sum_{t=1}^T \ell(p_t, q_t)
& \le - \ln \left( \frac{1}{2\sqrt{T}} \left( \frac{a_T}{T} \right)^{a_T} \left( \frac{b_T}{T} \right)^{b_T} \right) \\
& = \ln(2) + \frac{1}{2} \ln(T) - a_T \ln (p^*) - b_T \ln (1-p^*) \\
& = \ln(2) + \frac{1}{2} \ln(T) + \sum_{t=1}^T \ell(p^*, q_t) \; .
\end{align*}

\section{Online Log Loss Game}
\label{section:log-loss}

We consider a generalization of the data compression game where $q_t$ is not
restricted the set $\{0,1\}$ but it is allowed to be any real number in the
interval $[0,1]$. If in round $t$ we predict $p_t \in [0,1]$ and then receive
$q_t \in [0,1]$ the loss is
$$
\ell(p_t, q_t) = q_t \ln\left( \frac{1}{p_t}\right) + (1-q_t) \ln  \left( \frac{1}{1 - p_t} \right) \; .
$$

The formulas for $a_T, b_T$ and $p^*$ remain exactly the same as before
\begin{align*}
a_T & = \sum_{t=1}^T q_t \; , \\
b_T & = \sum_{t=1}^T (1 - q_t) = T - a_T \; , \\
p^* & =  \argmin_{p \in [0,1]} \sum_{t=1}^T \ell(p, q_t) = \frac{a_T}{a_T + b_T} = \frac{a_T}{T} = \frac{\sum_{t=1}^T q_t}{T} \; .
\end{align*}

The KT estimator is generalized in the obvious way
$$
p_t = \frac{\frac{1}{2} + \sum_{t=1}^{t-1} q_i}{t} = \frac{\frac{1}{2} + a_{t-1}}{t} \; .
$$

\subsection{Bound on loss of the KT Estimator}

Let $\widetilde q_1, \widetilde q_2, \dots, \widetilde q_T$ independent
Bernoulli variables with parameters $q_1, q_2, \dots, q_T$ respectively.
Let
\begin{align*}
\widetilde a_t & = \sum_{i=1}^t \widetilde q_i  \; , \\
\widetilde b_t & = t - \widetilde a_t \; , \\
\widetilde p_t & = \frac{\frac{1}{2} + \sum_{i=1}^{t-1} \widetilde q_i}{t} = \frac{\frac{1}{2} + \widetilde a_{t-1}}{t} \; , \\
\widehat p & = \frac{\widetilde a_T}{T} = \frac{\widetilde a_T}{\widetilde a_T + \widetilde b_T} \; . \\
\end{align*}
Clearly,
\begin{align*}
q_i & = \Exp[\widetilde q_i] \; , \\
a_t & = \Exp[\widetilde a_t] \; , \\
b_t & = \Exp[\widetilde b_t] \; , \\
p_t & = \Exp[\widetilde p_t] \; , \\
p^* & = \Exp[\widehat p] \; .
\end{align*}

Since $\ell(p,q)$ is linear in $q$ and convex in $p$, we have
\begin{align*}
\sum_{t=1}^T \ell(p_t, q_t)
& = \sum_{t=1}^T \ell( \Exp[ \widetilde p_t], \Exp[\widetilde q_t]) \\
& = \Exp\left[ \sum_{t=1}^T \ell( \Exp[ \widetilde p_t], \widetilde q_t) \right] \\
& \le \Exp\left[ \sum_{t=1}^T \ell( \widetilde p_t, \widetilde q_t) \right] \\
& \le - \Exp\left[ \ln \left\{ \frac{1}{2\sqrt{T}} \left( \frac{\widetilde a_T}{T} \right)^{\widetilde a_T} \left( \frac{\widetilde b_T}{T} \right)^{\widetilde b_T} \right\} \right] \\
& = \ln(2) + \frac{1}{2} \ln(T) + \Exp \left[ - \widetilde a_T \ln \left( \frac{\widetilde a_T}{T} \right) - \widetilde b_T \ln \left( \frac{\widetilde b_T}{T} \right) \right] \\
& = \ln(2) + \frac{1}{2} \ln(T) + T \cdot \Exp \left[ H(\widehat p) \right] \\
& \le \ln(2) + \frac{1}{2} \ln(T) + T \cdot H(\Exp[\widehat p]) \\
& = \ln(2) + \frac{1}{2} \ln(T) + T \cdot H(p^*)
\end{align*}
where $H(x) = - x \ln(x) - (1-x) \ln (1-x)$ is the binary entropy function.

\section{Online Optimization in One Dimension}
\label{section:online-linear-optimization-1D}

Consider an online game where each round we predict $w_t \in \R$
and the adversary chooses $g_t \in [-1,1]$. The algorithm tries to maximize
its cumulative gain $\sum_{t=1}^T g_t w_t$. The goal of algorithm
is to have small regret after $T$ rounds w.r.t. competitor $u \in \R$:
$$
\Regret_T(u) = \sum_{t=1}^t g_t u - \sum_{t=1}^T g_t w_t \; .
$$

We consider algorithms of certain form, which we call betting algorithms.  A
betting algorithm starts with $\Wealth_0 = 1$ dollars.
In every round, it chooses a ``fraction'' $\beta_t \in [-1,1]$ of its current
wealth to bet. The algorithm predicts
$$
w_t = \beta_t \cdot \Wealth_{t-1}
$$
where $\Wealth_t = 1 + \sum_{i=1}^t g_t w_t$. After making its prediction,
the algorithm's wealth is
$$
\Wealth_t
= g_t w_t + \Wealth_{t-1}
= g_t \beta_t \Wealth_{t-1} + \Wealth_t
= (1 + g_t\beta_t) \Wealth_{t-1} \; .
$$
Hence
$$
\Wealth_T = \prod_{t=1}^T (1 + g_t\beta_t) \; .
$$

\subsection{KT estimator for $\beta_t$}

We use KT estimator to define fractions $\beta_t$. Let
\begin{align*}
q_t & = \frac{1 + g_t}{2} \\
a_T & = \sum_{t=1}^T q_t \; , \\
b_T & = \sum_{t=1}^T (1 - q_t) = T - a_T \; , \\
p^* & =  \argmin_{p \in [0,1]} \sum_{t=1}^T \ell(p, q_t)
      = \frac{a_T}{a_T + b_T}
      = \frac{a_T}{T} = \frac{\sum_{t=1}^T q_t}{T} \; .
\end{align*}
We use the corresponding KT estimator
$$
p_t = \frac{\frac{1}{2} + \sum_{t=1}^{t-1} q_i}{t} = \frac{\frac{1}{2} + a_{t-1}}{t} = \frac{1}{2} + \frac{1}{2t} \sum_{i=1}^{t-1} g_i
$$
to define $\beta_t$ as
$$
\beta_t = 2p_t - 1 = \frac{1}{t} \sum_{i=1}^{t-1} g_i \; .
$$

\subsection{Wealth Lower Bound}

The following inequality will be useful
$$
\ln\left(1 + \beta g \right) \ge \left( \frac{1+g}{2} \right) \ln \left(1 + \beta\right) + \left( \frac{1-g}{2} \right) \ln \left(1 - \beta \right)
\qquad \text{for $g \in [-1,1]$ and $\beta \in (-1,1)$}.
$$

We lower bound logarithm of the wealth as follows
\allowdisplaybreaks
\begin{align*}
\ln \Wealth_T
& = \ln \prod_{t=1}^T (1 + g_t\beta_t) \\
& =  \sum_{t=1}^T \ln (1 + g_t\beta_t) \\
& \ge  \sum_{t=1}^T \left( \frac{1+g_t}{2} \right) \ln \left(1 + \beta_t\right) + \left( \frac{1-g_t}{2} \right) \ln \left(1 - \beta_t \right) \\
& =  \sum_{t=1}^T \left( \frac{1+g_t}{2} \right) \ln \left(2p_t \right) + \left( \frac{1-g_t}{2} \right) \ln \left(2 (1 - p_t) \right) \\
& =  T \ln(2) + \sum_{t=1}^T \left( \frac{1+g_t}{2} \right) \ln (p_t) + \left( \frac{1-g_t}{2} \right) \ln (1 - p_t) \\
& =  T \ln(2) - \sum_{t=1}^T \ell(p_t, q_t) \\
& \ge  T \ln(2) - \ln(2) - \frac{1}{2} \ln(T) - T \cdot H(p^*) \\
& =  - \ln(2) - \frac{1}{2} \ln(T) - T \cdot (H(p^*) + \ln(1/2)) \\
& =  - \ln(2) - \frac{1}{2} \ln(T) + T \cdot D\left(p^*, 1/2 \right)
\end{align*}
where $D(u,v) = u \ln(u/v) + (1-v) \ln((1-u)/(1-v))$ is the Kullback-Leibler divergence
between Bernoulli random variables with parameters $u,v$.

\section{Online Linear Optimization over Hilbert Space}
\label{section:online-linear-optimization-hilbert-space}

Consider an online game where each round we predict $w_t$ in a Hilbert space
$\H$ and the adversary chooses $g_t \in \H$ such that $\|g_t\| \le 1$. The
algorithm tries to maximize its cumulative gain $\sum_{t=1}^T \langle g_t, w_t
\rangle$. The goal of algorithm is to have small regret after $T$ rounds w.r.t.
competitor $u \in \H$:
$$
\Regret_T(u) = \sum_{t=1}^t \langle g_t, u \rangle - \sum_{t=1}^T \langle g_t, w_t \rangle \; .
$$

We consider algorithms of certain form, which we call betting algorithms.  A
betting algorithm starts with $\Wealth_0 = 1$ dollars.
In every round, it chooses a ``fraction'' $\beta_t \in \H$, $\|\beta\| \le 1$
of its current wealth to bet. The algorithm predicts
$$
w_t = \beta_t \cdot \Wealth_{t-1}
$$
where $\Wealth_t = 1 + \sum_{i=1}^t \langle g_t, w_t \rangle$. After making its prediction,
the algorithm's wealth is
$$
\Wealth_t = \langle g_t, w_t \rangle + \Wealth_{t-1} = \langle g_t, \beta_t \rangle \Wealth_{t-1} + \Wealth_t = (1 + \langle g_t, \beta_t \rangle) \Wealth_{t-1} \; .
$$
Hence
$$
\Wealth_T = \prod_{t=1}^T (1 + \langle g_t, \beta_t \rangle) \; .
$$

\subsection{KT estimator for $\beta_t$}

The algorithms chooses ``fraction''
$$
\beta_t = \frac{1}{t} \sum_{i=1}^{t-1} g_i \; .
$$


\subsection{Wealth Lower Bound}

\begin{lemma}[Extremes]
\label{lemma:extremes}
Let $h:(-a,a) \to \R$ be an even concave twice-differentiable function that
satisfies $x \cdot h''(x) \le h'(x)$ for all $x \in [0,a)$. Then, if vectors
$u,v \in \H$ satisfy $\|u\| + \|v\| < a$, then
\begin{equation}
\label{equation:lemma-extremes-1}
\langle u, v \rangle + h(\|u + v\|) \ge \min \left\{ \|u\| \cdot \|v\| + h(\|v\| + \|u\|), \ - \|u\| \cdot \|v\| + h(\|u\| - \|v\|) \right\} \; .
\end{equation}
Futhermore, for any $b \in [0,a]$ and any $u, v \in \H$ such that $\|v\| < a - b$ and $\|u\| \le b$,
\begin{equation}
\label{equation:lemma-extremes-2}
\langle u, v \rangle + h(\|u + v\|) \ge \min \left\{ b \cdot \|v\| + h(\|v\| + b), \ - b \cdot \|v\| + h(\|u\| - b) \right\} \; .
\end{equation}
\end{lemma}

\begin{proof}
If $u$ or $v$ is zero, the inequality \eqref{equation:lemma-extremes-1} clearly holds. From now on we assume that
$u,v$ are non-zero. Let $c$ be the cosine of the angle of between $u$ and $v$.
More formally,
$$
c = \frac{\langle u, v \rangle}{\|u\| \cdot \|v\|} \; .
$$
With this notation, the left-hand side is
$$
\langle u, v \rangle + h(\|u + v\|) = c \|u\| \cdot \|v\|  + h(\sqrt{\|u\|^2 + \|v\|^2 + 2 c \|u\| \cdot \|v\|}) \; .
$$
We consider the last expression as a function of $c$ and we call it $f(c)$. The
inequality \eqref{equation:lemma-extremes-1} is equivalent to
$$
\forall c \in [-1,1] \qquad \qquad f(c) \ge \min \left\{f(+1), f(-1)\right\} \; .
$$
The last inequality is clearly true if $f:[-1,1] \to \R$ is concave. We now
check that $f$ is indeed concave, which we prove by showing that the second
derivative is non-positive. The first derivate of $f$ is
$$
f'(c) = \|u\| \cdot \|v\| + \frac{h'(\sqrt{\|u\|^2 + \|v\|^2 + 2 c \|u\| \cdot \|v\|}) \cdot \|u\| \cdot \|v\|}{\sqrt{\|u\|^2 + \|v\|^2 + 2 c \|u\| \cdot \|v\|}} \; .
$$
The second derivative of $f$ is
$$
f''(c) = \|u\|^2 \cdot \|v\|^2 \cdot \frac{h''(\sqrt{\|u\|^2 + \|v\|^2 + 2 c \|u\| \cdot \|v\|})  - \frac{1}{\sqrt{\|u\|^2 + \|v\|^2 + 2c \|u\| \cdot \|v\|}} \cdot h'(\sqrt{\|u\|^2 + \|v\|^2 + 2 c \|u\| \cdot \|v\|})  }{\|u\|^2 + \|v\|^2 + 2 c \|u\| \cdot \|v\|} \; .
$$
If we consider $x=\sqrt{\|u\|^2 + \|v\|^2 + 2 c \|u\| \cdot \|v\|}$, the
assumption $x \cdot h''(x) \le h'(x)$ implies that $f''(c)$ is non-positive.
This finishes the proof of the inequality \eqref{equation:lemma-extremes-1}.

Inequality \eqref{equation:lemma-extremes-2}, can be derived from inequality
\eqref{equation:lemma-extremes-1} as follows
\begin{align*}
\langle u, v \rangle + h(\|u + v\|)
& \ge \min_{u \in \H : \|u\| \le b} \min \left\{ \|u\| \cdot \|v\| + h(\|v\| + b), \ - \|u\| \cdot \|v\| + h(\|u\| - \|v\|) \right\} \\
& = \min_{z \in [0,b]} \min \left\{ z \cdot \|v\| + h(\|v\| + z), \ - z \cdot \|v\| + h(\|u\| - z) \right\} \\
& = \min_{z \in [-b,b]} z \cdot \|v\| + h(\|v\| + z) \\
& = \min \left\{ b \cdot \|v\| + h(\|v\| + b), \ - b \cdot \|v\| + h(\|u\| - b) \right\} \; .
\end{align*}
The inequality in the chain is the inequality \eqref{equation:lemma-extremes-1}.
The last equality follows since $g(z) = z \cdot \|v\| + h(\|v\| + z)$
is concave.
\end{proof}

\begin{lemma}[Gamma Function]
\label{lemma:gamma-function}
For any real $a > 0$, the function $f(x) = \Gamma(a+x) \Gamma(a-x)$ satisfies
$$
\forall x \in [0, a) \qquad x \cdot f''(x) \ge f'(x) \; .
$$
\end{lemma}

\begin{proof}
First note that $f(x)$ is even, i.e., $f(x) = f(-x)$. Hence, odd terms of its
Maclaurin series are zero. The property $x \cdot f''(x) \ge f'(x)$ easily
follows if we show that the coefficients of the Maclaurin expansion of $f(x)$
around $0$ are non-negative, except possibly for the zero-order term $a_0$.
Indeed, if
$$
f(x) = \sum_{n=0}^\infty a_{2n} x^{2n} \qquad \text{and} \qquad a_2, a_4, a_6, \dots \ge 0
$$
then the condition $x \cdot f''(x) \ge f'(x)$ is equivalent to
$$
x \sum_{n=0}^\infty (2n)(2n-1) \cdot a_{2n} x^{2n-2} \ge \sum_{n=0}^\infty (2n) \cdot a_{2n} x^{2n-1} \; ,
$$
which holds for any $x \ge 0$ since each term on the right-hand side,
$(2n)(2n-1) a_{2n} x^{2n-1}$, is bigger or equal to the corresponding term on
the left-hand side, $(2n) a_{2n} x^{2n-1}$.

Since $\Gamma(x)$ is positive for any real $x > 0$, the function $f(x)$ is
positive on $(-a,a)$ and hence we can take its logarithm $g(x) = \ln(f(x))$.
Note that if $g(x)$ has non-negative (even) coefficients of its Maclaurin
expansion, except for possibly for the zero-order term, then the same holds for
$f(x) = \exp(g(x))$ since the coefficients of Maclaurin expansion $\exp(z) =
\sum_{n=0}^n \frac{z^n}{n!}$ are positive.

It thus remains to show that the coefficients of the Maclaurin expansion of
$g(x) = \ln(\Gamma(a+x) \Gamma(a-x))$ are non-negative. These coefficients can
be expressed in terms of \emph{logarithmic derivatives} of the Gamma function,
also called \emph{polygamma functions}. These are defined for any $n \ge 0$ as
any complex $x \in \C \setminus \N_{0}$ as
$$
\psi^{(n)}(x) = \frac{d^n\ln(\Gamma(x))}{dx^n} \; .
$$
The Mclaurin of expansion of $g(x)$ is
$$
g(x)
= \ln \left( \Gamma(a+x) \Gamma(a-x) \right)
= 2 \ln(\Gamma(a)) + \sum_{\substack{n \ge 2 \\ \text{$n$ even}}} \frac{\psi^{(n-1)}(a) \cdot x^n}{n!} \; .
$$
The fact that the coefficients $\psi^{(n-1)}(a)/n!$ are non-negative for even $n
\ge 2$ can be easily seen from the integral representation of polygamma
functions,
$$
\psi^{(n)}(z) = (-1)^{n+1} \int_0^\infty \frac{t^n e^{-zt}}{1-e^{-t}} dt \; ,
$$
valid for any $n \ge 1$ and any complex $z$ such that $\Re(z) > 0$.
\end{proof}

\begin{theorem}
For any sequence of $g_1, g_2, \dots, g_T \in \H$ such that $\|g_1\|, \|g_2\|,
\dots, \|g_T\| \le 1$, the algorithm predicting ``fraction'' $\beta_T =
\frac{1}{T} \sum_{t=1}^{T-1} g_t$ satisfies
$$
\Wealth_T
\ge
\frac{2^T \cdot \Gamma \left(\frac{T+1}{2} + \frac{1}{2}\left\|\sum_{t=1}^T g_t \right\| \right) \cdot \Gamma \left(\frac{T+1}{2} - \frac{1}{2} \left\|\sum_{t=1}^T g_t \right\| \right)}{\pi \cdot T!}
\; .
$$
\end{theorem}

\begin{proof}
We prove the inequality by induction on $T$. In the base case $T=0$, the
left-hand side is $\Wealth_0 = 1$, and the right-hand side equals $1$ as well
since $\sum_{t=1}^T g_t = 0$ and $\Gamma(\frac{1}{2}) = \sqrt{\pi}$. For $T \ge
1$, we will show that the difference of left and right side of the inequality is
non-negative. We have
\begin{align*}
& \Wealth_T \ - \ \frac{2^T \cdot \Gamma \left(\frac{T+1}{2} + \frac{1}{2}\left\|\sum_{t=1}^T g_t \right\| \right) \cdot \Gamma \left(\frac{T+1}{2} - \frac{1}{2} \left\|\sum_{t=1}^T g_t \right\| \right)}{\pi \cdot T!} \\
& = (1 + \langle g_T, \beta_T \rangle) \Wealth_{T-1} \ - \ \frac{2^T \cdot \Gamma \left(\frac{T+1}{2} + \frac{1}{2}\left\|\sum_{t=1}^T g_t \right\| \right) \cdot \Gamma \left(\frac{T+1}{2} - \frac{1}{2} \left\|\sum_{t=1}^T g_t \right\| \right)}{\pi \cdot T!} \\
& = \Wealth_{T-1} \ + \ \frac{1}{T} \cdot \Wealth_{T-1} \cdot \left(\langle g_T, T \beta_T \rangle - \frac{2^T T \cdot \Gamma \left(\frac{T+1}{2} + \frac{1}{2}\left\|\sum_{t=1}^T g_t \right\| \right) \cdot \Gamma \left(\frac{T+1}{2} - \frac{1}{2} \left\|\sum_{t=1}^T g_t \right\| \right)}{\Wealth_{T-1} \cdot \pi \cdot T!} \right) \\
& = \Wealth_{T-1} \ + \ \frac{1}{T} \cdot \Wealth_{T-1} \cdot \left( \left\langle g_T, \sum_{t=1}^{T-1} g_t \right\rangle + h\left( \left\|g_T + \sum_{t=1}^{T-1} g_t \right\| \right) \right) \; .
\end{align*}
where we define
\begin{align*}
h(x) & = - C \cdot \Gamma \left(\frac{T+1}{2} + \frac{x}{2} \right) \cdot \Gamma \left(\frac{T+1}{2} - \frac{x}{2} \right) \; ,  \\
C & = \frac{2^T \cdot T}{\Wealth_{T-1} \cdot \pi \cdot T!} \; .
\end{align*}
We apply Lemma~\ref{lemma:extremes} to $h(x)$. First note that $C > 0$. Second,
$\Gamma(x)$ is logarithmically convex on $[0,\infty)$, and hence $h(x)$ is
logarithmically concave and hence concave. Third, $h(x)$ is clearly even and its
defined on an interval $(-\frac{T+1}{2}, \frac{T+1}{2})$. To see that $h(x)$
satisfies the condition $x \cdot h''(x) \le h'(x)$, we apply
Lemma~\ref{lemma:gamma-function}. Let $f(x) = \Gamma(\frac{T+1}{2}+x)
\Gamma(\frac{T+1}{2}-x)$. By Lemma~\ref{lemma:gamma-function}, we have $x \cdot
f''(x) \ge f(x)$ for all $x \in [0,a)$. Since $h(x) = - C f(x/2)$, we have $x
\cdot h''(x) \le h'(x)$ for $x \in [0,T+1)$.

Therefore, since $\|g_t\| \le 1$ and $\|\sum_{t=1}^{T-1} g_t\| < T$,
\begin{align*}
& \Wealth_{T-1} \ + \ \frac{1}{T} \cdot \Wealth_{T-1} \cdot \left( \left\langle g_T, \sum_{t=1}^{T-1} g_t \right\rangle + h\left( \left\|g_T + \sum_{t=1}^{T-1} g_t \right\| \right) \right) \\
& \ge \Wealth_{T-1} \ + \ \frac{1}{T} \cdot \Wealth_{T-1} \cdot \min \left\{ \left\| \sum_{t=1}^{T-1} g_t \right\| + h\left( \left\|\sum_{t=1}^{T-1} g_t \right\| + 1 \right), - \left\| \sum_{t=1}^{T-1} g_t \right\| + h\left(\left\|\sum_{t=1}^{T-1} g_t \right\| - 1 \right) \right\} \\
& = \Wealth_{T-1} \ + \ \Wealth_{T-1} \cdot \min \left\{ \| \beta_T \| + \frac{1}{T} \cdot h\left( \left\|\sum_{t=1}^{T-1} g_t \right\| + 1 \right), - \|\beta_T\| + \frac{1}{T} \cdot h\left(\left\|\sum_{t=1}^{T-1} g_t \right\| - 1 \right) \right\} \\
& = \min \left\{ \Wealth_{T-1} (1 + \| \beta_T \|) + \frac{\Wealth_{T-1}}{T} h\left( \left\|\sum_{t=1}^{T-1} g_t \right\| + 1 \right), \Wealth_{T-1} (1 - \| \beta_T \|) + \frac{\Wealth_{T-1}}{T} h\left(\left\|\sum_{t=1}^{T-1} g_t \right\| - 1 \right) \right\}
\end{align*}
It remains to prove that the last expression is non-negative, which we do by
proving that each of the two sub-expressions of the minima are non-negative. The
first sub-expression is
\begin{align*}
& \Wealth_{T-1} (1 + \| \beta_T \|) + \frac{\Wealth_{T-1}}{T} h\left( \left\|\sum_{t=1}^{T-1} g_t \right\| + 1 \right) \\
& =  \Wealth_{T-1} (1 + \|\beta_T\|) - \frac{2^T \cdot \Gamma \left(\frac{T+1}{2} + \frac{1}{2}\left\|\sum_{t=1}^{T-1} g_t \right\| + \frac{1}{2} \right) \cdot \Gamma \left(\frac{T+1}{2} - \frac{1}{2} \left\|\sum_{t=1}^{T-1} g_t \right\| - \frac{1}{2} \right)}{\pi \cdot T!} \\
& =  \Wealth_{T-1} (1 + \|\beta_T\|) - \frac{2^T \left( \frac{T}{2} + \frac{1}{2} \left\|\sum_{t=1}^{T-1} g_t \right\| \right) \Gamma \left(\frac{T}{2} + \frac{1}{2}\left\|\sum_{t=1}^{T-1} g_t \right\| \right) \cdot \Gamma \left(\frac{T}{2} - \frac{1}{2} \left\|\sum_{t=1}^{T-1} g_t \right\| \right)}{\pi \cdot T!} \\
& =  \Wealth_{T-1} (1 + \|\beta_T\|) - \frac{2^{T-1} \left( 1 + \|\beta_T\| \right) \Gamma \left(\frac{T}{2} + \frac{1}{2}\left\|\sum_{t=1}^{T-1} g_t \right\| \right) \cdot \Gamma \left(\frac{T}{2} - \frac{1}{2} \left\|\sum_{t=1}^{T-1} g_t \right\| \right)}{\pi \cdot (T-1)!} \\
& =  (1 + \|\beta_T\|) \left( \Wealth_{T-1} - \frac{2^{T-1} \Gamma \left(\frac{T}{2} + \frac{1}{2}\left\|\sum_{t=1}^{T-1} g_t \right\| \right) \cdot \Gamma \left(\frac{T}{2} - \frac{1}{2} \left\|\sum_{t=1}^{T-1} g_t \right\| \right)}{\pi \cdot (T-1)!} \right) \; .
\end{align*}
The second sub-expression is
\begin{align*}
& \Wealth_{T-1} (1 - \| \beta_T \|) + \frac{\Wealth_{T-1}}{T} h\left(\left\|\sum_{t=1}^{T-1} g_t \right\| - 1 \right) \\
& = \Wealth_{T-1} (1 - \|\beta_T\|) - \frac{2^T \cdot \Gamma \left(\frac{T+1}{2} + \frac{1}{2}\left\|\sum_{t=1}^{T-1} g_t \right\| - \frac{1}{2} \right) \cdot \Gamma \left(\frac{T+1}{2} - \frac{1}{2} \left\|\sum_{t=1}^{T-1} g_t \right\| + \frac{1}{2} \right)}{\pi \cdot T!} \\
& = \Wealth_{T-1} (1 - \|\beta_T\|) - \frac{2^T \left( \frac{T}{2} - \frac{1}{2}\|\sum_{t=1}^{T-1} g_t\| \right) \cdot \Gamma \left(\frac{T}{2} + \frac{1}{2}\left\|\sum_{t=1}^{T-1} g_t \right\| \right) \cdot \Gamma \left(\frac{T}{2} - \frac{1}{2} \left\|\sum_{t=1}^{T-1} g_t \right\| \right)}{\pi \cdot T!} \\
& = \Wealth_{T-1} (1 - \|\beta_T\|) - \frac{2^{T-1} \left( 1 - \|\beta_T\| \right) \cdot \Gamma \left(\frac{T}{2} + \frac{1}{2}\left\|\sum_{t=1}^{T-1} g_t \right\| \right) \cdot \Gamma \left(\frac{T}{2} - \frac{1}{2} \left\|\sum_{t=1}^{T-1} g_t \right\| \right)}{\pi \cdot (T-1)!} \\
& = (1 - \|\beta_T\|) \left( \Wealth_{T-1}  - \frac{2^{T-1} \Gamma \left(\frac{T}{2} + \frac{1}{2}\left\|\sum_{t=1}^{T-1} g_t \right\| \right) \cdot \Gamma \left(\frac{T}{2} - \frac{1}{2} \left\|\sum_{t=1}^{T-1} g_t \right\| \right)}{\pi \cdot (T-1)!} \right) \; .
\end{align*}
In both cases we have used $\Gamma(x+1) = x \cdot \Gamma(x)$ which holds for any
real $x > 0$. Induction hypothesis and $\|\beta_T\| \in (-1,1)$ imply
that both sub-expressions are non-negative.
\end{proof}

\input{experts}

\end{document}
