\section{From Betting...}

In this section we will show the connection between betting on a continuous coin, adaptive \ac{OLO}/\ac{OCO}, and automatic model selection. We will prove that, given an ``optimal'' betting algorithm, the same algorithm can be used to solve all the problems listed above.

We will assume that a 1-dimensional algorithm exists that satisfies the following assumption.
\begin{assumption}
\label{assumption:1-d_algo}
Assume that there exists a function $f:\R \times 2^\R \rightarrow \R$ convex and twice differentiable in the first argument and an algorithm, that we will denote by \ac{MBA}, that generates $b_t \in [-1 , 1]$, using in input $x_{t-1} \in \R$, $z_1, \ldots, z_{t-1} \in [-1,1]$, such that
\begin{itemize}
\item $f$ is even in the first argument;
\item $f''(x, S) \geq \frac{f'(x,S)}{x}$, where the derivatives are w.r.t. the first argument;
\item $f(0,\{\})=\epsilon>0$;
\item  $
(1+b_t z_t) f\left( x_{t-1}, \{|z_1|, \ldots, |z_{t-1}|\} \right) \geq f\left( x_{t-1}+z_t, \{|z_1|, \ldots, |z_t|\}\right), \ \ \forall z_{t} \in [-1,1], |x_{t-1}| \leq \sum_{i=1}^{t-1} |z_i|
$.
\end{itemize}
\end{assumption}

We call it \ac{MBA} because it is easy to show by induction that such algorithm can indeed be used as a betting algorithm.
\begin{theorem}
\label{theo:1-d_reward}
Assume that Assumption \ref{assumption:1-d_algo} holds and use the \ac{MBA} with $x_{t-1}=\sum_{i=1}^{t-1} g_i$ and $z_t=g_t$.
Then, for any sequence of outcomes $g_t \in [-1,1]$, the following holds
\[
\wealth_T = \epsilon+\sum_{t=1}^T g_t b_t \geq f\left( \sum_{i=1}^{T} g_i , \left\{|g_1|, \ldots, |g_T|\right\}\right)~.
\]
\end{theorem}

In the following we will show how such an algorithm can be used to solve \ac{OLO} and learning with expert advice.

\subsection{...to Online Linear Learning}

Even more importantly, the \ac{MBA} can also be used to prove regret bound on \ac{OLO} in Hilbert Spaces.

\begin{theorem}
\label{theo:hilbert_reward}
  Assume that \ref{assumption:1-d_algo} holds.
  Let $g_t \in \fH$ an arbitrary sequence of vector, such that $\norm{g_t} \leq 1$ and define the vector $\theta_t=\sum_{i=1}^{t} g_i$.
  Use the Algorithm in \ref{assumption:1-d_algo} with the sequence $x_{t-1}= \norm{\theta_{t-1}}$.
  Define a vectorial algorithm that at each step outputs $w_t = b_t \frac{\theta_{t-1}}{\norm{\theta_{t-1}}} \wealth_{t-1}$. Then the following holds
  \[
    Regret_T(u) \leq f^*\left( \norm{u}, \left\{\norm{g_1}, \ldots, \norm{g_T}\right\}\right)+\epsilon \quad \quad \quad \textnormal{ for all } u \in \fH \textnormal{ and } g_t \in \fH,
  \]
  where the conjugation is taken w.r.t. the first argument of $f$.
  %\[
  %\wealth_{n} = \epsilon + \sum_{t=1}^n \langle g_t, w_t \rangle \geq f\left( \norm{\sum_{t=1}^n g_t}, \left\{\norm{g_1}, \ldots, \norm{g_n}\right\}\right)~.
  %\]
\end{theorem}

\subsection{...to Learning with Expert Advice}

In this setting we want to minimize the regret defined as
\[
\Regret(u) = \sum_{t=1}^T \langle p_t, \ell_t \rangle - \sum_{t=1}^T \langle u, \ell_t \rangle,
\]
where $\ell_t \in [0,1]$, $u, p_t \in \R^d_+$, and $\|u\|_1 = \|p_t\|_1 = 1$.

Suppose we have a one-dimensional online algorithm, that produces
predictions $w_t$ that guarantees
\[
\epsilon + \sum_{t=1}^T w_t g_t \ge \epsilon f\left(\sum_{t=1}^T g_t\right),
\]
for a given $f(\cdot)$. Then we can use the following prediction strategy for the expert setting.

Define $q_i$ a prior distribution over the $d$ experts. Instantiate the coin
betting algorithm $d$ times, setting $\epsilon =1$ and feed the $i$-th copy with
\begin{align}
g_{t,i} = \begin{cases}
\langle p_t, \ell_t\rangle - \ell_{t,i} & \text{if } w_{t,i} \ge 0 \\
|\langle p_t, \ell_t\rangle - \ell_{t,i}|_+ & \text{if } w_{t,i} < 0,
\end{cases}
\end{align}
where $p_{t,i} = \frac{\hat{p}_{t,i}}{\|p_{t}\|_1}$, $\hat{p}_{t,i}=q_i |w_{t,i}|_+$, and $|x|_+=\max\{x,0\}$.

The above definitions guarantee that $\sum_{i=1}^d q_i g_{t,i} w_{t,i} \le 0$. In fact, we have
\begin{align*}
\sum_{i=1}^d q_i g_{t,i} w_{t,i}
& = \sum_{i:w_{t,i}\ge 0} q_i w_{t,i} g_{t,i} + \sum_{i:w_{t,i} < 0} q_i w_{t,i} g_{t,i} \\
& = \sum_{i=1}^d q_i g_{t,i} |w_{t,i}|_+ + \sum_{i:w_{t,i} < 0} q_i w_{t,i} g_{t,i} \\
& = 0 + \sum_{i:w_{t,i} < 0} q_i w_{t,i} |\langle p_t, \ell_t\rangle - \ell_{t,i}|_+ \le 0
\end{align*}
Hence, for each $i$ we have
\[
1 + \sum_{t=1}^T w_{t,i} g_{t,i} \ge f \left( \sum_{t=1}^T g_{t,i} \right) \; .
\]
Multiplying each side by $q_i$ and summing over $i$, we have
\begin{equation}
\label{eq:bounded_potential}
\sum_{i=1}^d q_i f\left(\sum_{t=1}^T g_{t,i} \right) \le 1 + \sum_{i=1}^d q_i \sum_{t=1}^T w_{t,i} g_{t,i} \le 1~.
\end{equation}

We now use the specific form of $f$ given by Algorithm~\ref{}.
Define $f(x)= \exp(\frac{x^2}{A} - B)$ and $G_{T,i}=\sum_{t=1}^T g_{t,i}$.
We have
\begin{align*}
\Regret(u) & \le \langle u, G_T \rangle \\
& \le \sum_{i=1}^d u_i |G_{T,i}|_+ \\
& = \sum_{i=1}^d u_i \sqrt{A \left[\ln \exp\left(\frac{|G_{T,i}|_+^2}{A} - B\right)+B \right]} \\
& \le \sqrt{A \sum_{i=1}^d u_i \left[\ln \exp\left(\frac{|G_{T,i}|_+^2}{A} - B\right)+B \right]} \\
& = \sqrt{A \left\{B + \sum_{i=1}^d u_i \left[\ln \frac{q_i}{u_i}\exp\left(\frac{|G_{T,i}|_+^2}{A} - B \right) + \ln \frac{u_i}{q_i}\right]\right\}} \\
& = \sqrt{A \left\{B + \DKL(u||q) + \sum_{i=1}^d u_i \left[\ln \frac{q_i}{u_i}\exp\left(\frac{|G_{T,i}|_+^2}{A} - B\right)\right]\right\}} \\
& \le \sqrt{A \left\{B + \DKL(u||q) + \ln \sum_{i=1}^d u_i \left[ \frac{q_i}{u_i}\exp\left(\frac{|G_{T,i}|_+^2}{A} - B\right)\right]\right\}} \\
& = \sqrt{A \left\{B + \DKL(u||q) + \ln \sum_{i=1}^d q_i\exp\left(\frac{|G_{T,i}|_+^2}{A}-B\right)\right\}} \\
& \le \sqrt{A \left\{B + \DKL(u||q) + \ln \sum_{i=1}^d q_i\exp\left(\frac{G_{T,i}^2}{A}-B\right)\right\}} \\
& \le \sqrt{A \left(B + \DKL(u||q)\right)} \; ,
\end{align*}
where in the third and fourth inequality we used Jensen's, and in the last one
we used \eqref{eq:bounded_potential}.
