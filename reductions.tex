\section{From Betting...}

In this section we will show the connection between betting on a continuous coin, adaptive \ac{OLO}/\ac{OCO}, and automatic model selection. We will prove that, given an ``optimal'' betting algorithm, the same algorithm can be used to solve all the problems listed above.

We will assume that a 1-dimensional algorithm exists that satisfies the following assumption.
\begin{assumption}
\label{assumption:1-d_algo}
Assume that there exists a function $f:\R \times 2^\R \rightarrow \R$ convex and twice differentiable in the first argument, $\epsilon_0, \epsilon_1,\cdots, \epsilon_t>0$, and an algorithm, that we will denote by \ac{MBA}, that generates $b_t \in [-1 , 1]$, using in input $x_{t-1} \in \R$, $z_1, \ldots, z_{t-1} \in [-1,1]$, such that
\begin{itemize}
\item $f$ is even in the first argument;
\item $f''(x, S) \geq \frac{f'(x,S)}{x}$, where the derivatives are w.r.t. the first argument;
\item $f(0,\{\})=\epsilon_0>0$;
\item  $
(1+b_t z_t) f\left( x_{t-1}, \{|z_1|, \ldots, |z_{t-1}|\} \right) - f\left( x_{t-1}+z_t, \{|z_1|, \ldots, |z_t|\}\right) \geq -\epsilon_t, \ \ \forall z_{t} \in [-1,1], |x_{t-1}| \leq \sum_{i=1}^{t-1} |z_i|
$.
\end{itemize}
\end{assumption}
In the following we will use the abbreviated notation $f_t(x)$ to denote the function $f(x,\cdot)$ when the dependency on the set is only on its cardinality. We will denote this case as the data-independent case.

Even if the conditions are very general, it turns out that, when $\epsilon_t=0$, an optimal betting strategy is given by the choice of $f$.
\begin{theorem}
\label{theo:opt_beta}
Assume that the function $\phi(z)=f(x+z,\{S, |z|\})$, where $S$ is a set of non-negative numbers, is piece-wise logarithmically convex in $[-\infty,0)$ and $(0,\infty]$. Then, for any $z\in \R$,  the following choice of $b$
\[
b^* = \argmin_{b} \max_{0<|z|\leq 1} \ \frac{f(x+z,\{S, |z|\})}{1+b z} =\frac{f(x+1,\{S, 1\}) - f(x-1,\{S, 1\})}{f(x+1,\{S, 1\}) + f(x-1,\{S, 1\})}~.
\]
\end{theorem}


We call it \ac{MBA} because it is easy to show by induction that such algorithm can indeed be used as a betting algorithm.
The quantities $\epsilon_t, t=1,\cdots,T$ represents the loans that we ask to bet with leverage, while $\epsilon_0$ is our initial investement.
On each time step we bet a fraction, $|b_t|$, of your current wealth plus the loans on the event $\sign{g_t}==\sign(b_t)$.
The amount of money won or lost is equal to $|g_t|$ multiplied the amount of money bet.
Defining $w_t=b_t (\Wealth_{t-1}+\sum_{i=1}^{t-1} \epsilon_i)$, the money gained (or lost) in round $t$ is $g_t w_t$.
\begin{theorem}
\label{theo:1-d_reward}
Assume that Assumption \ref{assumption:1-d_algo} holds and use the \ac{MBA} with $x_{t-1}=\sum_{i=1}^{t-1} g_i$ and $z_t=g_t$.
Then, for any sequence of outcomes $g_t \in [-1,1]$, the following holds
\[
\wealth_T = \epsilon_0 + \sum_{t=1}^T g_t w_t \geq f\left( \sum_{i=1}^{T} g_i , \left\{|g_1|, \ldots, |g_T|\right\}\right)-\sum_{t=1}^T \epsilon_t~.
\]
\end{theorem}
This proof of this theorem of all the other proofs are in the Appendix.

In the following we will show how such an algorithm can be used to solve \ac{OLO} and learning with expert advice.

\subsection{...to Online Linear Learning}

Even more importantly, the \ac{MBA} can also be used to prove regret bound on \ac{OLO} in Hilbert Spaces.

\begin{theorem}
\label{theo:hilbert_reward}
  Assume that \ref{assumption:1-d_algo} holds.
  Let $g_t \in \fH$ an arbitrary sequence of vector, such that $\norm{g_t} \leq 1$ and define the vector $\theta_t=\sum_{i=1}^{t} g_i$.
  Use the Algorithm in \ref{assumption:1-d_algo} with the sequence $x_{t-1}= \norm{\theta_{t-1}}$.
  Define a vectorial algorithm that at each step outputs $w_t = b_t \frac{\theta_{t-1}}{\norm{\theta_{t-1}}} \wealth_{t-1}$. Then the following holds
  \[
    Regret_T(u) \leq f^*\left( \norm{u}, \left\{\norm{g_1}, \ldots, \norm{g_T}\right\}\right)+\sum_{t=0}^T \epsilon_t \quad \quad \quad \textnormal{ for all } u \in \fH \textnormal{ and } g_t \in \fH,
  \]
  where the conjugation is taken w.r.t. the first argument of $f$.
  %\[
  %\wealth_{n} = \epsilon + \sum_{t=1}^n \langle g_t, w_t \rangle \geq f\left( \norm{\sum_{t=1}^n g_t}, \left\{\norm{g_1}, \ldots, \norm{g_n}\right\}\right)~.
  %\]
\end{theorem}

\subsection{...to Learning with Expert Advice}

In this setting we want to minimize the regret defined as
\[
\Regret(u) = \sum_{t=1}^T \langle p_t, \ell_t \rangle - \sum_{t=1}^T \langle u, \ell_t \rangle,
\]
where $\ell_t \in [0,1]$, $u, p_t \in \R^d_+$, and $\|u\|_1 = \|p_t\|_1 = 1$.

Suppose we have a one-dimensional online algorithm, that produces
predictions $w_t$ that guarantees
\begin{align}
\label{eq:def_f}
\sum_{t=0}^T \epsilon_t + \sum_{t=1}^T w_t g_t \ge \exp\left(f_T\left(\sum_{t=1}^T g_t\right)\right),
\end{align}
for a given $f_T(\cdot)$. Then we can use the following prediction strategy for the expert setting.

Define $q_i$ a prior distribution over the $d$ experts. Instantiate the online algorithm
$d$ times, feeding each $i$-th copy with
\begin{align}
g_{t,i} = \begin{cases}
\langle p_t, \ell_t\rangle - \ell_{t,i} & \text{if } w_{t,i} \ge 0 \\
|\langle p_t, \ell_t\rangle - \ell_{t,i}|_+ & \text{if } w_{t,i} < 0,
\end{cases}
\end{align}
where $p_{t,i} = \frac{\hat{p}_{t,i}}{\|p_{t}\|_1}$, $\hat{p}_{t,i}=q_i |w_{t,i}|_+$, and $|x|_+=\max\{x,0\}$.

The above definitions guarantee that $\sum_{i=1}^d q_i g_{t,i} w_{t,i} \le 0$. In fact, we have
\begin{align*}
\sum_{i=1}^d q_i g_{t,i} w_{t,i}
& = \sum_{i:w_{t,i}\ge 0} q_i w_{t,i} g_{t,i} + \sum_{i:w_{t,i} < 0} q_i w_{t,i} g_{t,i} \\
& = \sum_{i=1}^d q_i g_{t,i} |w_{t,i}|_+ + \sum_{i:w_{t,i} < 0} q_i w_{t,i} g_{t,i} \\
& = 0 + \sum_{i:w_{t,i} < 0} q_i w_{t,i} |\langle p_t, \ell_t\rangle - \ell_{t,i}|_+ \le 0
\end{align*}
Hence, for each $i$ we have
\[
\sum_{t=0}^T \epsilon_t + \sum_{t=1}^T w_{t,i} g_{t,i} \geq \exp\left(f_T \left( \sum_{t=1}^T g_{t,i} \right)\right) \; .
\]
Multiplying each side by $q_i$ and summing over $i$, we have
\begin{equation}
\label{eq:bounded_potential}
\sum_{i=1}^d q_i \exp\left(f_T\left(\sum_{t=1}^T g_{t,i} \right)\right) \leq \sum_{t=0}^T \epsilon_t + \sum_{i=1}^d q_i \sum_{t=1}^T w_{t,i} g_{t,i} \le \sum_{t=0}^T \epsilon_t~.
\end{equation}

We can now state the following Theorem.
\begin{theorem}
\label{theo:expert_reduction}
Assume that the inverse of the function $f_T$ in \eqref{eq:def_f}, $f_T^{-1}$, is a concave function, and that $f$ is an increasing bijective function in $[0,\infty]$.
Then for the reduction gives a regret of 
\[
\Regret(u) = \sum_{t=1}^T \langle p_t, \ell_t \rangle - \sum_{t=1}^T \langle u, \ell_t \rangle \leq f_T^{-1}\left(\DKL(u||q) + \ln \sum_{t=0}^T \epsilon_t\right) \;.
\]
\end{theorem}

Also, as noted in \citet{LuoS15}, the above bound implies $\epsilon$-quantile bounds when $q$ is chosen as the uniform distribution, that is
\[
\Regret_T(u_{S_\epsilon}) \leq f_T^{-1}\left(\ln\left(\frac{1}{\epsilon}\right) + \ln \sum_{t=0}^T \epsilon_t\right) \; .
\]