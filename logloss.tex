\section{Online Log Loss Game}
\label{section:log-loss}

We consider a generalization of the data compression game where $q_t$ is not
restricted the set $\{0,1\}$ but it is allowed to be any real number in the
interval $[0,1]$. If in round $t$ we predict $p_t \in [0,1]$ and then receive
$q_t \in [0,1]$ the loss is
$$
\ell(p_t, q_t) = q_t \ln\left( \frac{1}{p_t}\right) + (1-q_t) \ln  \left( \frac{1}{1 - p_t} \right) \; .
$$

The KT estimator is generalized in the obvious way
$$
p_t = \frac{\frac{1}{2} + \sum_{t=1}^{t-1} q_i}{t}\; .
$$

%\subsection{Bound on loss of the KT Estimator}

In this setting we can prove the following Theorem.
\begin{theorem}
\label{theo:logloss}
For any sequence $q_1, q_2, \dots, q_T \in [0,1]$, the log-loss of KT
predictions is
$$
%\sum_{t=1}^T \ell(p_t, q_t) \leq \ln(2) + \frac{1}{2} \ln(T) + T \cdot H(p^*)
\sum_{t=1}^T \ell(p_t, q_t) \leq \ln(2) + \frac{1}{2} \ln(T) + T \ln (2) -T \, \DKL\left(p^*\middle\|\frac{1}{2}\right)
$$
where $H(x) = - x \ln(x) - (1-x) \ln (1-x)$ is the binary entropy function.
\end{theorem}